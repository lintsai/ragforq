# Q槽路徑設置
# Q_DRIVE_PATH="/mnt/winshare/MIC/MIC共用文件庫/05_MIC專案/20200310_IT Portal/"
# DISPLAY_DRIVE_NAME="Q:/MIC共用文件庫/05_MIC專案/20200310_IT Portal/"
Q_DRIVE_PATH="/mnt/winshare/MIC/"
DISPLAY_DRIVE_NAME="Q:/"

# 向量數據庫設置
VECTOR_DB_PATH="/app/db"
LOGS_DIR="/app/logs"
BACKUPS_DIR="/app/backups"

# 文件類型設置
SUPPORTED_FILE_TYPES=".pdf,.docx,.doc,.xlsx,.xls,.txt,.md,.pptx,.ppt,.csv,.vsdx,.vsd"

# 應用設置
APP_HOST=192.168.100.121
APP_PORT=8000
STREAMLIT_PORT=8501

# 其他設置
LOG_LEVEL="INFO"
MAX_TOKENS_CHUNK=500
CHUNK_OVERLAP=100
SIMILARITY_TOP_K=10

# 索引效能優化設置（針對大量文件優化）
MAX_WORKERS=1                           # 降低到單線程避免競爭
EMBEDDING_BATCH_SIZE=2                  # 進一步降低批次大小
FILE_BATCH_SIZE=3                       # 每次只處理3個文件
MAX_FILE_SIZE_MB=1000

# CUDA 記憶體管理設置 - 修復記憶體碎片化問題（生產環境）
PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,expandable_segments:False,garbage_collection_threshold:0.6"
CUDA_LAUNCH_BLOCKING=0
TORCH_CUDA_EMPTY_CACHE_FREQUENCY=10     # 每10次操作清理一次緩存
CUDA_VISIBLE_DEVICES="0,1"              # 可見的GPU設備

# 模型加載安全設置（生產環境）
MODEL_LOAD_MAX_RETRIES=3                # 生產環境增加重試次數
MODEL_LOAD_RETRY_DELAY=10               # 生產環境增加重試延遲
FORCE_MODEL_RELOAD=false                # 是否強制重新加載模型

# 智能回退模型配置（生產環境 - 可根據需要自定義）
FALLBACK_MEDIUM_MODEL="microsoft/DialoGPT-medium"  # 中型回退模型
FALLBACK_SMALL_MODEL="microsoft/DialoGPT-small"    # 小型回退模型
FALLBACK_TINY_MODEL="distilgpt2"                   # 微型回退模型

# 編碼處理優化
AUTO_ENCODING_DETECTION=true
USE_CHARDET=true
CLEAN_GARBLED_TEXT=true

# 動態RAG優化設置（針對大量文件保守配置）
DYNAMIC_MAX_SCAN_FILES=1000             # 降低掃描文件數量
DYNAMIC_SCAN_DEPTH=5                    # 降低掃描深度
DYNAMIC_CACHE_DURATION=600              # 增加緩存時間減少重複掃描
DYNAMIC_FOLDER_SELECTION=true           # 啟用文件夾選擇功能

# 文件夾瀏覽優化設置（生產環境）
FOLDER_SCAN_MAX_FILES=50                # 文件夾快速掃描最大文件數
FOLDER_SCAN_MAX_DEPTH=1                 # 文件夾掃描最大深度
FOLDER_SCAN_TIMEOUT=5                   # 文件夾掃描超時（秒）

# === Ollama 設置 ===
# 生產環境中的 Ollama 服務
OLLAMA_HOST="http://localhost:11434"

# Ollama 超時設定（秒）- 優化後
OLLAMA_REQUEST_TIMEOUT=300              # 一般請求超時（增加到5分鐘）
OLLAMA_EMBEDDING_TIMEOUT=180            # 嵌入向量生成超時（增加到3分鐘）
OLLAMA_QUERY_OPTIMIZATION_TIMEOUT=90    # 查詢優化超時
OLLAMA_ANSWER_GENERATION_TIMEOUT=300    # 回答生成超時（增加到5分鐘）
OLLAMA_RELEVANCE_TIMEOUT=120            # 相關性分析超時（增加到2分鐘）
OLLAMA_CONNECTION_TIMEOUT=60            # 連接超時（增加到1分鐘）

# 重試機制設定
OLLAMA_MAX_RETRIES=3                    # 最大重試次數
OLLAMA_RETRY_DELAY=5                    # 重試延遲（秒）

# === Hugging Face 設置 ===
HF_MODEL_CACHE_DIR="/app/models/cache"
HF_USE_GPU="true"
HF_TOKEN=""

# 模型設置 - 生產環境穩定配置
# 系統會根據GPU記憶體和錯誤情況智能選擇回退模型
# 生產環境建議使用經過測試的穩定模型，避免使用過大的實驗性模型
DEFAULT_LLM_MODEL="Qwen/Qwen2-7B-Instruct"
DEFAULT_EMBEDDING_MODEL="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# 推理引擎配置（生產環境穩定配置）
VLLM_GPU_MEMORY_UTILIZATION="0.7"      # 生產環境保守記憶體使用率
VLLM_MAX_MODEL_LEN="4096"               # 最大序列長度
VLLM_TENSOR_PARALLEL_SIZE="2"           # 雙GPU張量並行

# PyTorch 設置 - 生產環境優化記憶體管理
TORCH_DEVICE="auto"
TORCH_DTYPE="bfloat16"                  # 使用更穩定的數據類型

# 環境設置
ENVIRONMENT="production"

# 自訂的管理員安全密碼
ADMIN_TOKEN=ragadmin123