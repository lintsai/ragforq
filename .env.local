# Q槽路徑設置
Q_DRIVE_PATH="/ragforq/test_files/"
DISPLAY_DRIVE_NAME="C:/Users/user/source/ragforq/test_files/"

# 向量數據庫設置
VECTOR_DB_PATH="/app/vector_db"
LOGS_DIR="/app/logs"
BACKUPS_DIR="/app/backups"

# 文件類型設置
SUPPORTED_FILE_TYPES=".pdf,.docx,.doc,.xlsx,.xls,.txt,.md,.pptx,.ppt,.csv,.vsdx,.vsd"

# 應用設置
APP_HOST=127.0.0.1
APP_PORT=8000
STREAMLIT_PORT=8501

# 其他設置
LOG_LEVEL="INFO"
MAX_TOKENS_CHUNK=500
CHUNK_OVERLAP=100
SIMILARITY_TOP_K=10

# 索引效能優化設置（針對大量文件優化）
MAX_WORKERS=1
EMBEDDING_BATCH_SIZE=4
FILE_BATCH_SIZE=3
MAX_FILE_SIZE_MB=20

# CUDA 記憶體管理設置
PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:256,expandable_segments:True"
CUDA_LAUNCH_BLOCKING=0
TORCH_CUDA_EMPTY_CACHE_FREQUENCY=50

# 編碼處理優化
AUTO_ENCODING_DETECTION=true
USE_CHARDET=true
CLEAN_GARBLED_TEXT=true

# 動態RAG優化設置（測試環境）
DYNAMIC_MAX_SCAN_FILES=1000
DYNAMIC_SCAN_DEPTH=5
DYNAMIC_CACHE_DURATION=300
DYNAMIC_FOLDER_SELECTION=true

# === Ollama 設置 ===
# Docker 環境中連接到宿主機的 Ollama 服務
OLLAMA_HOST="http://host.docker.internal:11434"

# Ollama 超時設定（秒）- 優化後（測試環境）
OLLAMA_REQUEST_TIMEOUT=120              # 一般請求超時（測試環境較短）
OLLAMA_EMBEDDING_TIMEOUT=60             # 嵌入向量生成超時
OLLAMA_QUERY_OPTIMIZATION_TIMEOUT=60    # 查詢優化超時
OLLAMA_ANSWER_GENERATION_TIMEOUT=120    # 回答生成超時
OLLAMA_RELEVANCE_TIMEOUT=60             # 相關性分析超時
OLLAMA_CONNECTION_TIMEOUT=30            # 連接超時

# 重試機制設定
OLLAMA_MAX_RETRIES=3                    # 最大重試次數
OLLAMA_RETRY_DELAY=3                    # 重試延遲（秒）

# === Hugging Face 設置 ===
HF_MODEL_CACHE_DIR="/app/models/cache"
HF_USE_GPU="true"
HF_TOKEN=""

# 模型設置 - 根據 GPU 記憶體自動選擇
DEFAULT_LLM_MODEL="Qwen/Qwen2-0.5B-Instruct"
DEFAULT_EMBEDDING_MODEL="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# 推理引擎配置（通過前端設置流程選擇，以下僅為默認值）
VLLM_GPU_MEMORY_UTILIZATION="0.8"
VLLM_MAX_MODEL_LEN="4096"
VLLM_TENSOR_PARALLEL_SIZE="1"

# PyTorch 設置
TORCH_DEVICE="auto"
TORCH_DTYPE="float16"

# 環境設置
ENVIRONMENT="development"

# 自訂的管理員安全密碼
ADMIN_TOKEN=ragadmin123