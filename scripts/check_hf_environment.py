#!/usr/bin/env python
"""
Hugging Face ç’°å¢ƒæª¢æŸ¥è…³æœ¬
"""

import os
import sys
import subprocess
from pathlib import Path

def check_python_packages():
    """æª¢æŸ¥ Python åŒ…"""
    print("ğŸ“¦ æª¢æŸ¥ Python åŒ…...")
    
    required_packages = {
        "transformers": "4.35.0",
        "torch": "2.0.0", 
        "accelerate": "0.20.0",
        "datasets": "2.14.0",
        "sentence-transformers": "2.2.2",
        "langchain-huggingface": "0.1.0"
    }
    
    optional_packages = {
        "vllm": "0.2.0",
        "ray": "2.8.0",
        "tensorflow": "2.13.0"
    }
    
    print("  å¿…è¦åŒ…:")
    for package, min_version in required_packages.items():
        try:
            __import__(package)
            import importlib.metadata
            version = importlib.metadata.version(package)
            print(f"    âœ… {package} ({version})")
        except ImportError:
            print(f"    âŒ {package} - æœªå®‰è£")
        except Exception as e:
            print(f"    âš ï¸ {package} - æª¢æŸ¥å¤±æ•—: {str(e)}")
    
    print("  å¯é¸åŒ…:")
    for package, min_version in optional_packages.items():
        try:
            __import__(package)
            import importlib.metadata
            version = importlib.metadata.version(package)
            print(f"    âœ… {package} ({version})")
        except ImportError:
            print(f"    âš ï¸ {package} - æœªå®‰è£ï¼ˆå¯é¸ï¼‰")
        except Exception as e:
            print(f"    âš ï¸ {package} - æª¢æŸ¥å¤±æ•—: {str(e)}")

def check_gpu_support():
    """æª¢æŸ¥ GPU æ”¯æ´"""
    print("\nğŸ® æª¢æŸ¥ GPU æ”¯æ´...")
    
    # æª¢æŸ¥ NVIDIA GPU
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("  âœ… NVIDIA GPU å¯ç”¨")
            # è§£æ GPU ä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'GeForce' in line or 'RTX' in line or 'GTX' in line or 'Tesla' in line or 'A100' in line:
                    gpu_info = line.strip()
                    print(f"    ğŸ¯ GPU: {gpu_info}")
                    break
        else:
            print("  âŒ NVIDIA GPU ä¸å¯ç”¨æˆ–é©…å‹•æœªå®‰è£")
    except FileNotFoundError:
        print("  âŒ nvidia-smi å‘½ä»¤ä¸å­˜åœ¨")
    
    # æª¢æŸ¥ PyTorch GPU æ”¯æ´
    try:
        import torch
        if torch.cuda.is_available():
            print(f"  âœ… PyTorch CUDA æ”¯æ´: {torch.version.cuda}")
            print(f"  ğŸ”¢ GPU æ•¸é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                print(f"    GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("  âš ï¸ PyTorch CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU")
    except ImportError:
        print("  âŒ PyTorch æœªå®‰è£")
    except Exception as e:
        print(f"  âš ï¸ PyTorch GPU æª¢æŸ¥å¤±æ•—: {str(e)}")

def check_model_cache():
    """æª¢æŸ¥æ¨¡å‹ç·©å­˜ç›®éŒ„"""
    print("\nğŸ“ æª¢æŸ¥æ¨¡å‹ç·©å­˜...")
    
    # å¾ç’°å¢ƒè®Šæ•¸æˆ–é»˜èªè·¯å¾‘ç²å–ç·©å­˜ç›®éŒ„
    cache_dir = os.getenv("HF_MODEL_CACHE_DIR", "./models/cache")
    cache_path = Path(cache_dir)
    
    print(f"  ğŸ“‚ ç·©å­˜ç›®éŒ„: {cache_path.absolute()}")
    
    if cache_path.exists():
        print("  âœ… ç·©å­˜ç›®éŒ„å­˜åœ¨")
        
        # æª¢æŸ¥ç›®éŒ„å¤§å°
        total_size = 0
        file_count = 0
        for file_path in cache_path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        if file_count > 0:
            print(f"  ğŸ“Š ç·©å­˜å¤§å°: {total_size / 1e9:.2f}GB ({file_count} å€‹æ–‡ä»¶)")
        else:
            print("  ğŸ“Š ç·©å­˜ç›®éŒ„ç‚ºç©º")
    else:
        print("  âš ï¸ ç·©å­˜ç›®éŒ„ä¸å­˜åœ¨ï¼Œå°‡è‡ªå‹•å‰µå»º")
        try:
            cache_path.mkdir(parents=True, exist_ok=True)
            print("  âœ… ç·©å­˜ç›®éŒ„å‰µå»ºæˆåŠŸ")
        except Exception as e:
            print(f"  âŒ ç·©å­˜ç›®éŒ„å‰µå»ºå¤±æ•—: {str(e)}")
    
    # æª¢æŸ¥ç£ç›¤ç©ºé–“
    try:
        import shutil
        total, used, free = shutil.disk_usage(cache_path.parent)
        free_gb = free / 1e9
        print(f"  ğŸ’¾ å¯ç”¨ç£ç›¤ç©ºé–“: {free_gb:.1f}GB")
        
        if free_gb < 10:
            print("  âš ï¸ ç£ç›¤ç©ºé–“ä¸è¶³ï¼Œå»ºè­°è‡³å°‘ä¿ç•™ 50GB ç”¨æ–¼æ¨¡å‹ç·©å­˜")
        elif free_gb < 50:
            print("  âš ï¸ ç£ç›¤ç©ºé–“è¼ƒå°‘ï¼Œå»ºè­°ä¿ç•™æ›´å¤šç©ºé–“ç”¨æ–¼å¤§å‹æ¨¡å‹")
        else:
            print("  âœ… ç£ç›¤ç©ºé–“å……è¶³")
    except Exception as e:
        print(f"  âš ï¸ ç£ç›¤ç©ºé–“æª¢æŸ¥å¤±æ•—: {str(e)}")

def check_network_connectivity():
    """æª¢æŸ¥ç¶²è·¯é€£æ¥"""
    print("\nğŸŒ æª¢æŸ¥ç¶²è·¯é€£æ¥...")
    
    import requests
    
    # æª¢æŸ¥ Hugging Face Hub é€£æ¥
    try:
        response = requests.get("https://huggingface.co", timeout=10)
        if response.status_code == 200:
            print("  âœ… Hugging Face Hub å¯è¨ªå•")
        else:
            print(f"  âš ï¸ Hugging Face Hub éŸ¿æ‡‰ç•°å¸¸: {response.status_code}")
    except requests.exceptions.Timeout:
        print("  âš ï¸ Hugging Face Hub é€£æ¥è¶…æ™‚")
    except requests.exceptions.ConnectionError:
        print("  âŒ Hugging Face Hub é€£æ¥å¤±æ•—")
    except Exception as e:
        print(f"  âš ï¸ Hugging Face Hub é€£æ¥æª¢æŸ¥å¤±æ•—: {str(e)}")
    
    # æª¢æŸ¥ Hugging Face Token
    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        print("  âœ… Hugging Face Token å·²è¨­ç½®")
        # å¯ä»¥æ·»åŠ  Token é©—è­‰é‚è¼¯
    else:
        print("  âš ï¸ Hugging Face Token æœªè¨­ç½®ï¼ˆå¯é¸ï¼Œä½†å»ºè­°è¨­ç½®ä»¥æé«˜ä¸‹è¼‰é€Ÿåº¦ï¼‰")

def test_model_loading():
    """æ¸¬è©¦æ¨¡å‹è¼‰å…¥"""
    print("\nğŸ§ª æ¸¬è©¦æ¨¡å‹è¼‰å…¥...")
    
    try:
        from transformers import AutoTokenizer, AutoModel
        
        # æ¸¬è©¦è¼‰å…¥ä¸€å€‹å°å‹æ¨¡å‹
        model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        print(f"  ğŸ”„ æ¸¬è©¦è¼‰å…¥æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        
        print("  âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # æ¸¬è©¦æ¨ç†
        inputs = tokenizer("Hello, world!", return_tensors="pt")
        outputs = model(**inputs)
        
        print("  âœ… æ¨¡å‹æ¨ç†æˆåŠŸ")
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹è¼‰å…¥æ¸¬è©¦å¤±æ•—: {str(e)}")
        print("  ğŸ’¡ å»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥å’Œç£ç›¤ç©ºé–“")

def generate_recommendations():
    """ç”Ÿæˆå»ºè­°"""
    print("\nğŸ’¡ å»ºè­°å’Œä¸‹ä¸€æ­¥:")
    print("=" * 50)
    
    print("ğŸš€ å¦‚æœæ‰€æœ‰æª¢æŸ¥éƒ½é€šé:")
    print("  1. é‹è¡Œ python scripts/quick_start.py")
    print("  2. åœ¨å‰ç«¯é¸æ“‡ Hugging Face å¹³å°")
    print("  3. é¸æ“‡é©åˆçš„æ¨¡å‹çµ„åˆ")
    
    print("\nâš ï¸ å¦‚æœé‡åˆ°å•é¡Œ:")
    print("  1. GPU ä¸å¯ç”¨: ç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨ CPUï¼Œä½†é€Ÿåº¦è¼ƒæ…¢")
    print("  2. ç¶²è·¯å•é¡Œ: æª¢æŸ¥é˜²ç«ç‰†å’Œä»£ç†è¨­ç½®")
    print("  3. ç£ç›¤ç©ºé–“ä¸è¶³: æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶æˆ–æ“´å±•å„²å­˜")
    print("  4. åŒ…ç¼ºå¤±: é‹è¡Œ pip install -r requirements.txt")
    
    print("\nğŸ“š åƒè€ƒæ–‡æª”:")
    print("  â€¢ docs/huggingface_setup.md - è©³ç´°è¨­ç½®æŒ‡å—")
    print("  â€¢ README.md - ç³»çµ±æ¦‚è¦½")
    
    print("\nğŸ”§ é€²éšé…ç½®:")
    print("  â€¢ è¨­ç½® HF_TOKEN ä»¥æé«˜ä¸‹è¼‰é€Ÿåº¦")
    print("  â€¢ èª¿æ•´ INFERENCE_ENGINE é¸æ“‡æ¨ç†å¼•æ“")
    print("  â€¢ é…ç½® GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” Hugging Face ç’°å¢ƒæª¢æŸ¥å·¥å…·")
    print("=" * 60)
    
    check_python_packages()
    check_gpu_support()
    check_model_cache()
    check_network_connectivity()
    test_model_loading()
    generate_recommendations()
    
    print("\n" + "=" * 60)
    print("âœ… ç’°å¢ƒæª¢æŸ¥å®Œæˆï¼")

if __name__ == "__main__":
    main()
