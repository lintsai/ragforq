"""
Dynamic RAG Engine Base - å‹•æ…‹æª¢ç´¢å¢å¼·ç”Ÿæˆå¼•æ“çš„åŸºåº•é¡åˆ¥
"""

import os
import logging
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

from langchain_core.documents import Document
from utils.hf_langchain_wrapper import HuggingFaceEmbeddings, ChatHuggingFace
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np

from .interfaces import RAGEngineInterface
from utils.file_parsers import FileParser
from config.config import (
    MAX_TOKENS_CHUNK, CHUNK_OVERLAP,
    SUPPORTED_FILE_TYPES, Q_DRIVE_PATH,
    OLLAMA_REQUEST_TIMEOUT
)

logger = logging.getLogger(__name__)

class SmartFileRetriever:
    """æ™ºèƒ½æ–‡ä»¶æª¢ç´¢å™¨ - æ ¹æ“šæŸ¥è©¢å…§å®¹æ™ºèƒ½é¸æ“‡ç›¸é—œæ–‡ä»¶"""
    
    def __init__(self, base_path: str = Q_DRIVE_PATH, folder_path: Optional[str] = None):
        self.base_path = base_path
        self.folder_path = folder_path  # æŒ‡å®šçš„æ–‡ä»¶å¤¾è·¯å¾‘éæ¿¾
        self.file_cache = {}  # æ–‡ä»¶å…ƒæ•¸æ“šç·©å­˜
        self.last_scan_time = 0
        self.cache_duration = 300  # 5åˆ†é˜ç·©å­˜
    
    def retrieve_relevant_files(self, query: str, max_files: int = 10) -> List[str]:
        """
        æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            max_files: æœ€å¤§è¿”å›æ–‡ä»¶æ•¸
            
        Returns:
            ç›¸é—œæ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        """
        # æ›´æ–°æ–‡ä»¶ç·©å­˜
        self._update_file_cache()

        # å¦‚æœç·©å­˜ä¸­çš„æ–‡ä»¶ç¸½æ•¸å¾ˆå°‘ï¼Œå‰‡ç›´æ¥è¿”å›æ‰€æœ‰æ–‡ä»¶
        if len(self.file_cache) <= 50:
            logger.info(f"Found only {len(self.file_cache)} files, processing all of them.")
            return list(self.file_cache.keys())
        
        # 1. é—œéµè©åŒ¹é…
        keyword_matches = self._match_by_keywords(query)
        
        # 2. è·¯å¾‘èªç¾©åˆ†æ
        path_matches = self._analyze_file_paths(query)
        
        # 3. åˆä½µå’Œå»é‡
        all_matches = list(set(keyword_matches + path_matches))
        
        # 4. è©•åˆ†å’Œæ’åº
        scored_files = self._score_files(query, all_matches)
        
        # 5. è¿”å›å‰Nå€‹æ–‡ä»¶
        return [file_path for file_path, score in scored_files[:max_files]]
    
    def _update_file_cache(self):
        """æ›´æ–°æ–‡ä»¶ç·©å­˜ - å„ªåŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå®Œæ•´Qæ§½æƒæå’Œæ–‡ä»¶å¤¾éæ¿¾"""
        current_time = time.time()
        if current_time - self.last_scan_time < self.cache_duration:
            return
        
        # ç¢ºå®šæƒæè·¯å¾‘
        if self.folder_path:
            scan_path = os.path.join(self.base_path, self.folder_path.lstrip('/'))
            logger.info(f"æ›´æ–°æ–‡ä»¶ç·©å­˜ï¼ˆé™åˆ¶åœ¨æ–‡ä»¶å¤¾: {self.folder_path}ï¼‰...")
        else:
            scan_path = self.base_path
            logger.info("æ›´æ–°æ–‡ä»¶ç·©å­˜...")
        
        self.file_cache = {}
        
        try:
            # æ ¹æ“šæ˜¯å¦æœ‰æ–‡ä»¶å¤¾é™åˆ¶èª¿æ•´æƒæåƒæ•¸
            if self.folder_path:
                # æ–‡ä»¶å¤¾é™åˆ¶æ¨¡å¼ï¼šå¯ä»¥æƒææ›´å¤šæ–‡ä»¶
                max_files = 10000
                max_depth = 10
            else:
                # å…¨å±€æƒææ¨¡å¼ï¼šä¿å®ˆè¨­ç½®
                max_files = 5000
                max_depth = 8
            
            file_count = 0
            
            # å„ªå…ˆæƒæé‡è¦ç›®éŒ„
            priority_dirs = []
            common_dirs = []
            
            # åˆ†é¡ç›®éŒ„
            try:
                # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶å¤¾è·¯å¾‘ï¼Œç›´æ¥æƒæè©²è·¯å¾‘
                if self.folder_path:
                    # æª¢æŸ¥æŒ‡å®šçš„æ–‡ä»¶å¤¾æ˜¯å¦å­˜åœ¨
                    if os.path.exists(scan_path) and os.path.isdir(scan_path):
                        priority_dirs = [scan_path]
                        logger.info(f"ç›´æ¥æƒææŒ‡å®šæ–‡ä»¶å¤¾: {scan_path}")
                    else:
                        logger.warning(f"æŒ‡å®šçš„æ–‡ä»¶å¤¾ä¸å­˜åœ¨: {scan_path}")
                        priority_dirs = [self.base_path]
                else:
                    # å…¨å±€æƒææ¨¡å¼ï¼Œåˆ†é¡å­ç›®éŒ„
                    for item in os.listdir(scan_path):
                        item_path = os.path.join(scan_path, item)
                        if os.path.isdir(item_path):
                            item_lower = item.lower()
                            # å„ªå…ˆè™•ç†å¯èƒ½åŒ…å«é‡è¦æ–‡æª”çš„ç›®éŒ„
                            if any(keyword in item_lower for keyword in ['æ–‡ä»¶', 'è³‡æ–™', 'æª”æ¡ˆ', 'document', 'data', 'file', 'å…±ç”¨', 'share']):
                                priority_dirs.append(item_path)
                            else:
                                common_dirs.append(item_path)
            except Exception as e:
                logger.warning(f"åˆ—èˆ‰ç›®éŒ„æ™‚å‡ºéŒ¯: {str(e)}")
                priority_dirs = [scan_path if self.folder_path else self.base_path]
            
            # æŒ‰å„ªå…ˆç´šæƒæ
            scan_dirs = priority_dirs + common_dirs
            if not scan_dirs:
                scan_dirs = [scan_path if self.folder_path else self.base_path]
            
            for scan_dir in scan_dirs:
                if file_count >= max_files:
                    break
                    
                try:
                    for root, dirs, files in os.walk(scan_dir):
                        # è¨ˆç®—ç•¶å‰æ·±åº¦ï¼ˆç›¸å°æ–¼æƒæèµ·å§‹è·¯å¾‘ï¼‰
                        if self.folder_path:
                            # æ–‡ä»¶å¤¾é™åˆ¶æ¨¡å¼ï¼šç›¸å°æ–¼æŒ‡å®šæ–‡ä»¶å¤¾è¨ˆç®—æ·±åº¦
                            depth = root.replace(scan_path, '').count(os.sep)
                        else:
                            # å…¨å±€æ¨¡å¼ï¼šç›¸å°æ–¼åŸºç¤è·¯å¾‘è¨ˆç®—æ·±åº¦
                            depth = root.replace(self.base_path, '').count(os.sep)
                            
                        if depth >= max_depth:
                            dirs[:] = []  # ä¸å†æ·±å…¥å­ç›®éŒ„
                            continue
                        
                        # è·³éç³»çµ±ç›®éŒ„å’Œéš±è—ç›®éŒ„
                        dirs[:] = [d for d in dirs if not d.startswith('.') and d.lower() not in ['system', 'temp', 'tmp', '$recycle.bin']]
                        
                        # å¢åŠ æ¯å€‹ç›®éŒ„çš„æ–‡ä»¶è™•ç†æ•¸é‡
                        files = files[:200]  # æ¯å€‹ç›®éŒ„æœ€å¤šè™•ç†200å€‹æ–‡ä»¶
                        
                        for file in files:
                            if file_count >= max_files:
                                logger.info(f"é”åˆ°æ–‡ä»¶æ•¸é‡é™åˆ¶ ({max_files})ï¼Œåœæ­¢æƒæ")
                                break
                                
                            # è·³éç³»çµ±æ–‡ä»¶å’Œè‡¨æ™‚æ–‡ä»¶
                            if file.startswith('.') or file.startswith('~') or file.lower().endswith('.tmp'):
                                continue
                                
                            file_path = os.path.join(root, file)
                            file_ext = os.path.splitext(file)[1].lower()
                            
                            if file_ext in SUPPORTED_FILE_TYPES:
                                try:
                                    stat = os.stat(file_path)
                                    # è·³ééå°çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ç©ºæ–‡ä»¶ï¼‰
                                    if stat.st_size < 100:  # å°æ–¼100å­—ç¯€
                                        continue
                                        
                                    # è¨ˆç®—ç›¸å°è·¯å¾‘
                                    if self.folder_path:
                                        # æ–‡ä»¶å¤¾é™åˆ¶æ¨¡å¼ï¼šé¡¯ç¤ºç›¸å°æ–¼æŒ‡å®šæ–‡ä»¶å¤¾çš„è·¯å¾‘
                                        relative_path = os.path.relpath(file_path, scan_path)
                                        display_path = os.path.join(self.folder_path, relative_path).replace('\\', '/')
                                    else:
                                        # å…¨å±€æ¨¡å¼ï¼šé¡¯ç¤ºç›¸å°æ–¼åŸºç¤è·¯å¾‘çš„è·¯å¾‘
                                        display_path = os.path.relpath(file_path, self.base_path).replace('\\', '/')
                                    
                                    self.file_cache[file_path] = {
                                        'name': file,
                                        'size': stat.st_size,
                                        'mtime': stat.st_mtime,
                                        'ext': file_ext,
                                        'relative_path': display_path,
                                        'depth': depth,
                                        'folder_limited': bool(self.folder_path)  # æ¨™è¨˜æ˜¯å¦ç‚ºæ–‡ä»¶å¤¾é™åˆ¶æ¨¡å¼
                                    }
                                    file_count += 1
                                except (OSError, PermissionError):
                                    continue
                        
                        if file_count >= max_files:
                            break
                except Exception as e:
                    logger.warning(f"æƒæç›®éŒ„ {scan_dir} æ™‚å‡ºéŒ¯: {str(e)}")
                    continue
            
            self.last_scan_time = current_time
            logger.info(f"æ–‡ä»¶ç·©å­˜æ›´æ–°å®Œæˆï¼Œå…± {len(self.file_cache)} å€‹æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡ä»¶ç·©å­˜å¤±æ•—: {str(e)}")
            # å¦‚æœæƒæå¤±æ•—ï¼Œè‡³å°‘å˜—è©¦æƒææ ¹ç›®éŒ„
            try:
                logger.info("å˜—è©¦åƒ…æƒææ ¹ç›®éŒ„...")
                for file in os.listdir(self.base_path)[:50]:  # åªæƒææ ¹ç›®éŒ„çš„å‰50å€‹æ–‡ä»¶
                    file_path = os.path.join(self.base_path, file)
                    if os.path.isfile(file_path):
                        file_ext = os.path.splitext(file)[1].lower()
                        if file_ext in SUPPORTED_FILE_TYPES:
                            try:
                                stat = os.stat(file_path)
                                self.file_cache[file_path] = {
                                    'name': file,
                                    'size': stat.st_size,
                                    'mtime': stat.st_mtime,
                                    'ext': file_ext,
                                    'relative_path': file
                                }
                            except (OSError, PermissionError):
                                continue
                logger.info(f"æ ¹ç›®éŒ„æƒæå®Œæˆï¼Œæ‰¾åˆ° {len(self.file_cache)} å€‹æ–‡ä»¶")
            except Exception as fallback_error:
                logger.error(f"æ ¹ç›®éŒ„æƒæä¹Ÿå¤±æ•—: {str(fallback_error)}")
                # å¦‚æœå®Œå…¨å¤±æ•—ï¼Œå‰µå»ºä¸€å€‹ç©ºç·©å­˜
                self.file_cache = {}
    
    def _match_by_keywords(self, query: str) -> List[str]:
        """åŸºæ–¼é—œéµè©åŒ¹é…æ–‡ä»¶"""
        query_lower = query.lower()
        query_words = query_lower.split()
        
        matches = []
        for file_path, metadata in self.file_cache.items():
            file_name_lower = metadata['name'].lower()
            path_lower = metadata['relative_path'].lower()
            
            # è¨ˆç®—åŒ¹é…åˆ†æ•¸
            score = 0
            for word in query_words:
                # å®Œæ•´è©åŒ¹é…
                if word in file_name_lower:
                    score += 2  # æ–‡ä»¶ååŒ¹é…æ¬Šé‡æ›´é«˜
                if word in path_lower:
                    score += 1  # è·¯å¾‘åŒ¹é…
                
                # éƒ¨åˆ†è©åŒ¹é…ï¼ˆæ›´å¯¬é¬†çš„åŒ¹é…ï¼‰
                for file_word in file_name_lower.split():
                    if word in file_word or file_word in word:
                        score += 1
            
            # å¦‚æœæ²’æœ‰é—œéµè©åŒ¹é…ï¼Œä½†æŸ¥è©¢å¾ˆçŸ­ï¼Œå‰‡åŒ…å«æ‰€æœ‰æ–‡ä»¶
            if score == 0 and len(query_words) <= 2:
                score = 0.1  # çµ¦ä¸€å€‹å¾ˆå°çš„åˆ†æ•¸
            
            if score > 0:
                matches.append((file_path, score))
        
        # æŒ‰åˆ†æ•¸æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return [file_path for file_path, score in matches]
    
    def _analyze_file_paths(self, query: str) -> List[str]:
        """åŸºæ–¼è·¯å¾‘èªç¾©åˆ†æåŒ¹é…æ–‡ä»¶"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šåŸºæ–¼è·¯å¾‘é—œéµè©
        path_keywords = {
            'æ”¿ç­–': ['policy', 'regulation', 'æ”¿ç­–', 'è¦å®š'],
            'æµç¨‹': ['process', 'procedure', 'æµç¨‹', 'ç¨‹åº'],
            'æ‰‹å†Š': ['manual', 'handbook', 'æ‰‹å†Š', 'æŒ‡å—'],
            'å ±å‘Š': ['report', 'å ±å‘Š', 'åˆ†æ'],
            'åˆç´„': ['contract', 'agreement', 'åˆç´„', 'å”è­°']
        }
        
        matches = []
        query_lower = query.lower()
        
        for category, keywords in path_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                for file_path, metadata in self.file_cache.items():
                    path_lower = metadata['relative_path'].lower()
                    if any(keyword in path_lower for keyword in keywords):
                        matches.append(file_path)
        
        return matches
    
    def _score_files(self, query: str, file_paths: List[str]) -> List[Tuple[str, float]]:
        """ç‚ºæ–‡ä»¶è©•åˆ†ä¸¦æ’åº"""
        scored_files = []
        
        for file_path in file_paths:
            if file_path not in self.file_cache:
                continue
                
            metadata = self.file_cache[file_path]
            score = 0
            
            # æ–‡ä»¶åç›¸é—œæ€§
            file_name_lower = metadata['name'].lower()
            query_lower = query.lower()
            
            # é—œéµè©åŒ¹é…åˆ†æ•¸
            query_words = query_lower.split()
            for word in query_words:
                if word in file_name_lower:
                    score += 2
                if word in metadata['relative_path'].lower():
                    score += 1
            
            # æ–‡ä»¶é¡å‹æ¬Šé‡
            type_weights = {
                '.pdf': 1.2,
                '.docx': 1.1,
                '.txt': 1.0,
                '.md': 1.0,
                '.xlsx': 0.9,
                '.pptx': 0.8
            }
            score *= type_weights.get(metadata['ext'], 1.0)
            
            # æ–‡ä»¶å¤§å°æ¬Šé‡ï¼ˆé©ä¸­å¤§å°çš„æ–‡ä»¶å¯èƒ½åŒ…å«æ›´å¤šæœ‰ç”¨ä¿¡æ¯ï¼‰
            size_kb = metadata['size'] / 1024
            if 10 <= size_kb <= 1000:  # 10KB - 1MB
                score *= 1.1
            elif size_kb > 5000:  # å¤§æ–¼5MBçš„æ–‡ä»¶é™æ¬Š
                score *= 0.8
            
            # æœ€è¿‘ä¿®æ”¹æ™‚é–“æ¬Šé‡
            days_old = (time.time() - metadata['mtime']) / (24 * 3600)
            if days_old < 30:  # 30å¤©å…§çš„æ–‡ä»¶
                score *= 1.1
            elif days_old > 365:  # è¶…éä¸€å¹´çš„æ–‡ä»¶
                score *= 0.9
            
            scored_files.append((file_path, score))
        
        # æŒ‰åˆ†æ•¸æ’åº
        scored_files.sort(key=lambda x: x[1], reverse=True)
        return scored_files


class DynamicContentProcessor:
    """å‹•æ…‹å…§å®¹è™•ç†å™¨ - å³æ™‚è§£æå’Œè™•ç†æ–‡ä»¶å…§å®¹"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=MAX_TOKENS_CHUNK,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
        )
        self.content_cache = {}  # å…§å®¹ç·©å­˜
        self.cache_duration = 600  # 10åˆ†é˜ç·©å­˜
    
    def process_files(self, file_paths: List[str]) -> List[Document]:
        """
        ä¸¦è¡Œè™•ç†å¤šå€‹æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
            
        Returns:
            è™•ç†å¾Œçš„æ–‡æª”åˆ—è¡¨
        """
        documents = []
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_file = {
                executor.submit(self._process_single_file, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_documents = future.result()
                    documents.extend(file_documents)
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ä»¶ {file_path} å¤±æ•—: {str(e)}")
        
        return documents
    
    def _process_single_file(self, file_path: str) -> List[Document]:
        """è™•ç†å–®å€‹æ–‡ä»¶"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = self._get_cache_key(file_path)
        if cache_key in self.content_cache:
            cache_entry = self.content_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_duration:
                return cache_entry['documents']
        
        try:
            # ç²å–è§£æå™¨
            parser = FileParser.get_parser_for_file(file_path)
            if not parser:
                logger.warning(f"ç„¡æ³•ç²å–æ–‡ä»¶è§£æå™¨: {file_path}")
                return []
            
            # è§£ææ–‡ä»¶å…§å®¹
            content_blocks = parser.safe_parse(file_path)
            if not content_blocks:
                return []
            
            # å‰µå»ºæ–‡æª”
            documents = []
            for text, metadata in content_blocks:
                if not text or not text.strip():
                    continue
                
                # åˆ†å‰²æ–‡æœ¬
                chunks = self.text_splitter.split_text(text)
                
                for i, chunk in enumerate(chunks):
                    chunk_metadata = metadata.copy()
                    chunk_metadata.update({
                        "chunk_id": i,
                        "chunk_count": len(chunks),
                        "file_path": file_path
                    })
                    
                    doc = Document(
                        page_content=chunk,
                        metadata=chunk_metadata
                    )
                    documents.append(doc)
            
            # ç·©å­˜çµæœ
            self.content_cache[cache_key] = {
                'documents': documents,
                'timestamp': time.time()
            }
            
            return documents
            
        except Exception as e:
            logger.error(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {str(e)}")
            return []
    
    def _get_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        try:
            stat = os.stat(file_path)
            content = f"{file_path}_{stat.st_mtime}_{stat.st_size}"
            return hashlib.md5(content.encode()).hexdigest()
        except OSError:
            return hashlib.md5(file_path.encode()).hexdigest()


class RealTimeVectorizer:
    """å³æ™‚å‘é‡åŒ–å¼•æ“"""
    
    def __init__(self, embedding_model: str, platform: str = "ollama"):
        # æ ¹æ“šå¹³å°é¸æ“‡åµŒå…¥æ¨¡å‹
        if platform == "ollama":
            from utils.ollama_embeddings import OllamaEmbeddings
            self.embeddings = OllamaEmbeddings(model_name=embedding_model)
            logger.info(f"ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹: {embedding_model}")
        else: # é»˜èªç‚º huggingface
            self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
            logger.info(f"ä½¿ç”¨ Hugging Face åµŒå…¥æ¨¡å‹: {embedding_model}")
        
        self.query_cache = {}  # æŸ¥è©¢å‘é‡ç·©å­˜
        self.cache_duration = 1800  # 30åˆ†é˜ç·©å­˜
    
    def vectorize_query(self, query: str) -> np.ndarray:
        """å‘é‡åŒ–æŸ¥è©¢"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.query_cache:
            cache_entry = self.query_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_duration:
                return cache_entry['vector']
        
        try:
            vector = self.embeddings.embed_query(query)
            vector_array = np.array(vector)
            
            # ç·©å­˜çµæœ
            self.query_cache[cache_key] = {
                'vector': vector_array,
                'timestamp': time.time()
            }
            
            return vector_array
        except Exception as e:
            logger.error(f"æŸ¥è©¢å‘é‡åŒ–å¤±æ•—: {str(e)}")
            return np.array([])
    
    def vectorize_documents(self, documents: List[Document]) -> List[Tuple[Document, np.ndarray]]:
        """å‘é‡åŒ–æ–‡æª”"""
        doc_vectors = []
        
        try:
            # æ‰¹é‡å‘é‡åŒ–
            texts = [doc.page_content for doc in documents]
            vectors = self.embeddings.embed_documents(texts)
            
            for doc, vector in zip(documents, vectors):
                doc_vectors.append((doc, np.array(vector)))
                
        except Exception as e:
            logger.error(f"æ–‡æª”å‘é‡åŒ–å¤±æ•—: {str(e)}")
        
        return doc_vectors
    
    def calculate_similarities(self, query_vector: np.ndarray, 
                             doc_vectors: List[Tuple[Document, np.ndarray]]) -> List[Tuple[Document, float]]:
        """è¨ˆç®—ç›¸ä¼¼åº¦"""
        if len(query_vector) == 0:
            return []
        
        similarities = []
        
        for doc, doc_vector in doc_vectors:
            if len(doc_vector) == 0:
                continue
                
            try:
                # æ‰‹å‹•è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(query_vector, doc_vector)
                similarities.append((doc, similarity))
            except Exception as e:
                logger.error(f"è¨ˆç®—ç›¸ä¼¼åº¦å¤±æ•—: {str(e)}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        æ‰‹å‹•è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: ç¬¬ä¸€å€‹å‘é‡
            vec2: ç¬¬äºŒå€‹å‘é‡
            
        Returns:
            é¤˜å¼¦ç›¸ä¼¼åº¦å€¼
        """
        try:
            # è¨ˆç®—é»ç©
            dot_product = np.dot(vec1, vec2)
            
            # è¨ˆç®—å‘é‡çš„æ¨¡é•·
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            # é¿å…é™¤é›¶éŒ¯èª¤
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            similarity = dot_product / (norm1 * norm2)
            
            # ç¢ºä¿çµæœåœ¨ [-1, 1] ç¯„åœå…§
            return float(np.clip(similarity, -1.0, 1.0))
            
        except Exception as e:
            logger.error(f"è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {str(e)}")
            return 0.0


class DynamicRAGEngineBase(RAGEngineInterface):
    """å‹•æ…‹RAGå¼•æ“åŸºåº•é¡åˆ¥ - åŒ…å«å…±é€šé‚è¼¯"""

    REWRITE_PROMPT_TEMPLATE = ""
    ANSWER_PROMPT_TEMPLATE = ""
    RELEVANCE_PROMPT_TEMPLATE = ""
    
    def __init__(self, ollama_model: str, ollama_embedding_model: str, platform: str = "ollama", folder_path: Optional[str] = None):
        self.ollama_model = ollama_model
        self.ollama_embedding_model = ollama_embedding_model
        self.platform = platform
        self.folder_path = folder_path
        
        # åˆå§‹åŒ–çµ„ä»¶ï¼Œå‚³éæ–‡ä»¶å¤¾è·¯å¾‘
        self.file_retriever = SmartFileRetriever(folder_path=folder_path)
        self.content_processor = DynamicContentProcessor()
        self.vectorizer = RealTimeVectorizer(ollama_embedding_model, platform=self.platform)
        
        # åˆå§‹åŒ–èªè¨€æ¨¡å‹
        try:
            if self.platform == "ollama":
                from langchain_ollama import OllamaLLM
                from config.config import OLLAMA_HOST
                self.llm = OllamaLLM(
                    model=ollama_model,
                    base_url=OLLAMA_HOST,
                    temperature=0.1,
                    timeout=OLLAMA_REQUEST_TIMEOUT,
                    request_timeout=OLLAMA_REQUEST_TIMEOUT
                )
                logger.info(f"ä½¿ç”¨ Ollama èªè¨€æ¨¡å‹: {ollama_model}")
            else: # é»˜èªç‚º huggingface
                self.llm = ChatHuggingFace(
                    model_name=ollama_model,
                    temperature=0.1
                )
                logger.info(f"ä½¿ç”¨ Hugging Face èªè¨€æ¨¡å‹ (é€é ModelManager): {ollama_model}")
        except ImportError:
            logger.warning("langchain_ollama æœªå®‰è£ï¼ŒOllama æ¨¡å‹å°‡é€é Hugging Face åŒ…è£å™¨è™•ç†")
            self.llm = ChatHuggingFace(
                model_name=ollama_model,
                temperature=0.1
            )
        except Exception as e:
            logger.error(f"èªè¨€æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            logger.info("å›é€€åˆ°ä½¿ç”¨ ChatHuggingFace ä½œç‚ºæœ€çµ‚æ–¹æ¡ˆ")
            self.llm = ChatHuggingFace(
                model_name=ollama_model,
                temperature=0.1
            )

        # é¡¯å¼è¨˜éŒ„æœ€çµ‚èªè¨€èˆ‡å­é¡ï¼Œé¿å…èª¤è§£ç‚ºç›´æ¥ä½¿ç”¨ base
        try:
            logger.info(f"Dynamic RAG Engine åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {ollama_model}ï¼Œèªè¨€: {self.get_language()}ï¼Œå¼•æ“: {self.__class__.__name__}")
        except Exception:
            logger.info(f"Dynamic RAG Engine åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {ollama_model}")
        
        logger.info(f"Dynamic RAG Engine åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {ollama_model}")
    
    def rewrite_query(self, original_query: str) -> str:
        """æŸ¥è©¢é‡å¯« - é‡å°å‹•æ…‹æª¢ç´¢å„ªåŒ–"""
        try:
            if len(original_query.strip()) <= 3:
                return original_query
            
            prompt = self.REWRITE_PROMPT_TEMPLATE.format(original_query=original_query)
            response = self.llm.invoke(prompt)
            
            rewritten_query = response.content.strip() if hasattr(response, 'content') else str(response).strip()

            if not rewritten_query or len(rewritten_query) < 2:
                return original_query
            
            punctuation_count = sum(1 for char in rewritten_query if char in 'ï¼Œ,ã€‚.ï¼!ï¼Ÿ?ï¼›;ï¼š:')
            if punctuation_count > len(rewritten_query) * 0.3:
                return original_query
            
            if len(rewritten_query) > len(original_query) * 2:
                return original_query

            logger.info(f"å„ªåŒ–å¾ŒæŸ¥è©¢: {rewritten_query}")
            return rewritten_query
            
        except Exception as e:
            logger.error(f"æŸ¥è©¢é‡å¯«å¤±æ•—: {str(e)}")
            return original_query
    
    def answer_question(self, question: str) -> str:
        """å›ç­”å•é¡Œ - å‹•æ…‹RAGæµç¨‹ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            logger.info(f"é–‹å§‹å‹•æ…‹RAGè™•ç†: {question}")
            
            # 1. æŸ¥è©¢é‡å¯«å„ªåŒ–
            optimized_query = self.rewrite_query(question)
            
            # 2. æª¢ç´¢ç›¸é—œæ–‡ä»¶ï¼ˆå¢åŠ æ•¸é‡ï¼‰
            relevant_files = self.file_retriever.retrieve_relevant_files(optimized_query, max_files=12)
            
            if not relevant_files:
                return self._generate_general_knowledge_answer(question)
            
            # 3. è™•ç†æ–‡ä»¶å…§å®¹
            documents = self.content_processor.process_files(relevant_files)
            
            if not documents:
                return self._generate_general_knowledge_answer(question)
            
            # 4. å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è¨ˆç®—
            query_vector = self.vectorizer.vectorize_query(optimized_query)
            doc_vectors = self.vectorizer.vectorize_documents(documents)
            similarities = self.vectorizer.calculate_similarities(query_vector, doc_vectors)
            
            # 5. ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–‡æª”é¸æ“‡ç­–ç•¥
            top_docs = self._select_best_documents(similarities, question)
            
            if not top_docs:
                return self._generate_general_knowledge_answer(question)
            
            # 6. ç”Ÿæˆè±å¯Œçš„ä¸Šä¸‹æ–‡
            context = self._format_enhanced_context(top_docs)
            answer = self._generate_answer(question, context)
            
            return answer
            
        except Exception as e:
            logger.error(f"å‹•æ…‹RAGè™•ç†å¤±æ•—: {str(e)}")
            return self._get_error_message()
    
    def _select_best_documents(self, similarities: List[Tuple[Document, float]], question: str) -> List[Document]:
        """
        æ™ºèƒ½é¸æ“‡æœ€ä½³æ–‡æª”
        
        Args:
            similarities: æ–‡æª”ç›¸ä¼¼åº¦åˆ—è¡¨
            question: åŸå§‹å•é¡Œ
            
        Returns:
            é¸ä¸­çš„æ–‡æª”åˆ—è¡¨
        """
        if not similarities:
            return []
        
        # æŒ‰æ–‡ä»¶åˆ†çµ„ï¼Œæ¯å€‹æ–‡ä»¶åªä¿ç•™æœ€ç›¸é—œçš„æ®µè½
        file_groups = {}
        for doc, score in similarities:
            file_path = doc.metadata.get('file_path', 'unknown')
            if file_path not in file_groups or score > file_groups[file_path][1]:
                file_groups[file_path] = (doc, score)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        sorted_docs = sorted(file_groups.values(), key=lambda x: x[1], reverse=True)
        
        # å‹•æ…‹é–¾å€¼é¸æ“‡
        if sorted_docs:
            best_score = sorted_docs[0][1]
            threshold = max(best_score * 0.7, 0.3)  # å‹•æ…‹é–¾å€¼
            
            selected_docs = []
            for doc, score in sorted_docs:
                if score >= threshold and len(selected_docs) < 8:  # æœ€å¤š8å€‹æ–‡æª”
                    selected_docs.append(doc)
            
            logger.info(f"é¸æ“‡äº† {len(selected_docs)} å€‹æ–‡æª”ï¼Œé–¾å€¼: {threshold:.3f}")
            return selected_docs
        
        return []
    
    def _format_enhanced_context(self, documents: List[Document]) -> str:
        """
        æ ¼å¼åŒ–å¢å¼·ä¸Šä¸‹æ–‡ - åƒè€ƒå‚³çµ±RAGçš„åšæ³•
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        """
        context_parts = []
        max_total_length = 4000  # é™åˆ¶ç¸½é•·åº¦
        current_length = 0
        
        for i, doc in enumerate(documents, 1):
            file_name = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            content = doc.page_content.strip()
            
            if content and current_length < max_total_length:
                # è¨ˆç®—å¯ç”¨é•·åº¦
                available_length = max_total_length - current_length
                if len(content) > available_length:
                    content = content[:available_length] + "..."
                
                context_parts.append(f"ç›¸é—œå…§å®¹ {i} (ä¾†æº: {file_name}):\n{content}\n")
                current_length += len(content)
                
                if current_length >= max_total_length:
                    break
        
        return "\n".join(context_parts)
    
    def _format_context(self, documents: List[Document]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            file_name = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            content = doc.page_content.strip()
            context_parts.append(f"ç›¸é—œå…§å®¹ {i} (ä¾†æº: {file_name}):\n{content}\n")
        
        return "\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            if self.llm is None:
                return f"æ ¹æ“šæ–‡æª”å…§å®¹ï¼Œé—œæ–¼ã€Œ{question}ã€çš„ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n{context[:500]}...\n\næ³¨æ„ï¼šDynamic RAG èªè¨€æ¨¡å‹æš«æ™‚ç¦ç”¨ï¼Œé€™æ˜¯åŸºæ–¼æª¢ç´¢å…§å®¹çš„ç°¡åŒ–å›ç­”ã€‚"
            
            prompt = self.ANSWER_PROMPT_TEMPLATE.format(context=context, question=question)
            
            response = self.llm.invoke(prompt)
            result = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            # æª¢æŸ¥å›ç­”é•·åº¦ï¼Œå¦‚æœå¤ªçŸ­å‰‡ä½¿ç”¨å›é€€æ–¹æ³•
            if not result or len(result.strip()) < 5:
                return self._get_general_fallback(question)
            
            # å˜—è©¦ç¢ºä¿è¼¸å‡ºç¬¦åˆç›®æ¨™èªè¨€
            return self._ensure_language(result)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return self._get_error_message()
    
    def _generate_general_knowledge_answer(self, question: str) -> str:
        """ç”Ÿæˆå¸¸è­˜å›ç­” - ç•¶æ‰¾ä¸åˆ°ç›¸é—œæ–‡æª”æ™‚ï¼ŒåŸºæ–¼å¸¸è­˜æä¾›å›ç­”ï¼ˆå­é¡å¯¦ç¾ï¼‰"""
        # é»˜èªå¯¦ç¾ï¼Œå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•
        return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æª”ä¸­æ‰¾ä¸åˆ°èˆ‡ã€Œ{question}ã€ç›¸é—œçš„å…·é«”ä¿¡æ¯ã€‚é€™å¯èƒ½æ˜¯å› ç‚ºç›¸é—œæ–‡æª”ä¸åœ¨ç•¶å‰çš„æª¢ç´¢ç¯„åœå…§ï¼Œæˆ–è€…å•é¡Œæ¶‰åŠçš„å…§å®¹éœ€è¦æ›´å…·é«”çš„é—œéµè©ã€‚å»ºè­°æ‚¨å˜—è©¦ä½¿ç”¨æ›´å…·é«”çš„é—œéµè©é‡æ–°æå•ã€‚"
    
    def _get_general_fallback(self, query: str) -> str:
        """ç²å–é€šç”¨å›é€€å›ç­”ï¼ˆå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•ï¼‰"""
        # é»˜èªå¯¦ç¾ï¼Œå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•
        return f"æ ¹æ“šä¸€èˆ¬ITçŸ¥è­˜ï¼Œé—œæ–¼ã€Œ{query}ã€çš„ç›¸é—œä¿¡æ¯å¯èƒ½éœ€è¦æŸ¥é–±æ›´å¤šQSIå…§éƒ¨æ–‡æª”ã€‚"
    
    def generate_relevance_reason(self, question: str, doc_content: str) -> str:
        """ç”Ÿæˆç›¸é—œæ€§ç†ç”±"""
        if not question or not question.strip() or not doc_content or not doc_content.strip():
            return "ç„¡æ³•ç”Ÿæˆç›¸é—œæ€§ç†ç”±ï¼šæŸ¥è©¢æˆ–æ–‡æª”ç‚ºç©º"

        try:
            trimmed_content = doc_content[:1000].strip()
            prompt = self.RELEVANCE_PROMPT_TEMPLATE.format(question=question, trimmed_content=trimmed_content)
            response = self.llm.invoke(prompt)
            reason = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            return reason if reason else "ç„¡æ³•ç¢ºå®šç›¸é—œæ€§ç†ç”±"
        except Exception as e:
            logger.error(f"ç”Ÿæˆç›¸é—œæ€§ç†ç”±æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç”Ÿæˆç›¸é—œæ€§ç†ç”±å¤±æ•—"

    def generate_batch_relevance_reasons(self, question: str, documents: List[Document]) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆç›¸é—œæ€§ç†ç”±"""
        reasons = []
        for doc in documents:
            reasons.append(self.generate_relevance_reason(question, doc.page_content))
        return reasons
    
    def _get_no_docs_message(self) -> str:
        return "æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”"
    
    def _get_error_message(self) -> str:
        return "ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\n\nğŸ’¡ é€™å¯èƒ½æ˜¯å› ç‚ºæ¨¡å‹å°šæœªå®Œå…¨ä¸‹è¼‰æˆ–åˆå§‹åŒ–ã€‚å¦‚æœæ‚¨æ˜¯é¦–æ¬¡ä½¿ç”¨ï¼Œè«‹ç­‰å¾…æ¨¡å‹ä¸‹è¼‰å®Œæˆå¾Œå†è©¦ã€‚\n\nå»ºè­°ï¼š\n- æª¢æŸ¥ç¶²è·¯é€£æ¥\n- é¸æ“‡è¼ƒå°çš„æ¨¡å‹é€²è¡Œæ¸¬è©¦\n- æŸ¥çœ‹ç³»çµ±ç‹€æ…‹ç¢ºèªæ¨¡å‹æ˜¯å¦å°±ç·’"
    
    def _get_timeout_message(self) -> str:
        return "è™•ç†è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦"
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Document]:
        """å‹•æ…‹æª¢ç´¢æ–‡æª”"""
        try:
            relevant_files = self.file_retriever.retrieve_relevant_files(query, max_files=top_k * 2)
            documents = self.content_processor.process_files(relevant_files)
            
            if not documents:
                return []
            
            query_vector = self.vectorizer.vectorize_query(query)
            doc_vectors = self.vectorizer.vectorize_documents(documents)
            similarities = self.vectorizer.calculate_similarities(query_vector, doc_vectors)
            
            return [doc for doc, score in similarities[:top_k] if score > 0.2]
            
        except Exception as e:
            logger.error(f"å‹•æ…‹æª¢ç´¢æ–‡æª”å¤±æ•—: {str(e)}")
            return []

    def _ensure_language(self, text: str) -> str:
        """åœ¨å¿…è¦æ™‚å°‡è¼¸å‡ºè½‰æ›ç‚ºç›®æ¨™èªè¨€ï¼Œç¢ºä¿æœ€çµ‚å›ç­”èªè¨€ä¸€è‡´"""
        try:
            target_lang = None
            try:
                target_lang = self.get_language()
            except Exception:
                target_lang = None

            if not target_lang or not text or len(text) < 5:
                return text

            # ç°¡å–®èªè¨€ç‰¹å¾µçµ±è¨ˆ
            total_len = max(1, len(text))
            ascii_letters = sum(1 for c in text if c.isascii() and c.isalpha())
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            thai_chars = sum(1 for c in text if '\u0e00' <= c <= '\u0e7f')

            ascii_ratio = ascii_letters / total_len
            zh_ratio = chinese_chars / total_len
            th_ratio = thai_chars / total_len

            def translate(to_lang_prompt: str) -> str:
                try:
                    translate_prompt = f"{to_lang_prompt}\n\nâ€”â€”â€”\n{text}\nâ€”â€”â€”\nåªè¼¸å‡ºç¿»è­¯çµæœã€‚"
                    resp = self.llm.invoke(translate_prompt)
                    return resp.content.strip() if hasattr(resp, 'content') else str(resp).strip()
                except Exception:
                    return text

            if target_lang in ("ç¹é«”ä¸­æ–‡", "ç®€ä½“ä¸­æ–‡"):
                # ä¸»è¦ç‚ºä¸­æ–‡ï¼Œè‹¥ä¸­æ–‡æ¯”ä¾‹éä½ä¸”è‹±æ–‡æ¯”ä¾‹é«˜ï¼Œå˜—è©¦ç¿»è­¯
                if zh_ratio < 0.20 and ascii_ratio > 0.50:
                    return translate("è«‹å°‡ä»¥ä¸‹å…§å®¹ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡") if target_lang == "ç¹é«”ä¸­æ–‡" else translate("è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡")
                return text

            if target_lang == "English":
                # ä¸»è¦ç‚ºè‹±æ–‡ï¼Œè‹¥è‹±æ–‡æ¯”ä¾‹éä½ä½†ä¸­æ–‡æˆ–æ³°æ–‡æ¯”ä¾‹è¼ƒé«˜ï¼Œå˜—è©¦ç¿»è­¯
                if ascii_ratio < 0.40 and (zh_ratio > 0.20 or th_ratio > 0.20):
                    return translate("Please translate the following content into English")
                return text

            if target_lang == "à¹„à¸—à¸¢":
                if th_ratio < 0.10 and (ascii_ratio > 0.50 or zh_ratio > 0.20):
                    return translate("à¹‚à¸›à¸£à¸”à¹à¸›à¸¥à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢")
                return text

            return text
        except Exception:
            return text
    
    
