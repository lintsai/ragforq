"""
Dynamic RAG Engine Base - å‹•æ…‹æª¢ç´¢å¢å¼·ç”Ÿæˆå¼•æ“çš„åŸºåº•é¡åˆ¥
"""

import os
import logging
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

from langchain_core.documents import Document
from utils.hf_langchain_wrapper import HuggingFaceEmbeddings, ChatHuggingFace
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np

from .interfaces import RAGEngineInterface
from utils.file_parsers import FileParser
from config.config import (
    MAX_TOKENS_CHUNK, CHUNK_OVERLAP,
    SUPPORTED_FILE_TYPES, Q_DRIVE_PATH,
    OLLAMA_REQUEST_TIMEOUT
)

logger = logging.getLogger(__name__)

# å®‰å…¨äº‹ä»¶è¿½è¹¤ (ç›®éŒ„è¶Šç•Œç­‰)
SECURITY_EVENTS: list[dict] = []  # [{ts:int, original:str, resolved:str, reason:str}]
LAST_SECURITY_ALERT_TS: Optional[int] = None  # æœ€è¿‘ä¸€æ¬¡å·²å¯«å…¥è­¦å ±æ™‚é–“ï¼ˆé¿å…é‡è¤‡åˆ·å¯«ï¼‰

def _write_security_alert(message: str):
    """å°‡å®‰å…¨è­¦å ±å¯«å…¥ç¨ç«‹æ—¥èªŒæª”æ¡ˆã€‚"""
    try:
        from config.config import LOGS_DIR
        os.makedirs(LOGS_DIR, exist_ok=True)
        path = os.path.join(LOGS_DIR, 'security_alerts.log')
        with open(path, 'a', encoding='utf-8') as f:
            f.write(message + '\n')
    except Exception:
        pass

def record_security_event(original: str, resolved: str, reason: str):
    """è¨˜éŒ„æ½›åœ¨çš„è·¯å¾‘å®‰å…¨äº‹ä»¶ï¼ˆä¾‹å¦‚è¶Šç•Œå˜—è©¦ï¼‰ã€‚"""
    try:
        from time import time as _now
        now_ts = int(_now())
        SECURITY_EVENTS.append({
            'ts': int(_now()),
            'original': original,
            'resolved': resolved,
            'reason': reason
        })
        # ä¿æŒæœ€è¿‘ 200 ç­†
        if len(SECURITY_EVENTS) > 200:
            del SECURITY_EVENTS[:-200]
        # äº‹ä»¶é€Ÿç‡æª¢æ¸¬ï¼šæœ€è¿‘ 10 åˆ†é˜é”åˆ° critical/elevated è§¸ç™¼ä¸€æ¬¡è­¦å ±
        recent_10m = 0
        for ev in reversed(SECURITY_EVENTS):
            if now_ts - ev['ts'] > 600:
                break
            recent_10m += 1
        # è‡¨ç•Œæ¢ä»¶
        level = None
        if recent_10m >= 15:
            level = 'critical'
        elif recent_10m >= 8:
            level = 'elevated'
        global LAST_SECURITY_ALERT_TS
        if level:
            # åƒ…ç•¶è·é›¢ä¸Šæ¬¡ alert è¶…é 120 ç§’æ‰å¯«å…¥ï¼Œä»¥é™ä½å™ªéŸ³
            if (LAST_SECURITY_ALERT_TS is None) or (now_ts - LAST_SECURITY_ALERT_TS > 120):
                _write_security_alert(f"ts={now_ts} level={level} recent_10m={recent_10m} last_event_reason={reason} original={original} resolved={resolved}")
                LAST_SECURITY_ALERT_TS = now_ts
    except Exception:
        pass

class SmartFileRetriever:
    """æ™ºèƒ½æ–‡ä»¶æª¢ç´¢å™¨ - æ ¹æ“šæŸ¥è©¢å…§å®¹æ™ºèƒ½é¸æ“‡ç›¸é—œæ–‡ä»¶"""

    def __init__(self, base_path: str = Q_DRIVE_PATH, folder_path: Optional[str] = None):
        """åˆå§‹åŒ–æ–‡ä»¶æª¢ç´¢å™¨"""
        # åŸºæœ¬å±¬æ€§
        self.base_path = Path(base_path).resolve()
        # å®‰å…¨åŒ–è™•ç† folder_pathï¼Œé˜²æ­¢è¶Šç•Œï¼ˆç›®éŒ„ç©¿è¶Šï¼‰
        if folder_path:
            try:
                candidate = Path(self.base_path, folder_path).resolve()
                if not str(candidate).startswith(str(self.base_path)):
                    logger.warning(f"[FolderScopeSecurity] éæ³•çš„è³‡æ–™å¤¾è·¯å¾‘è¶Šç•Œå˜—è©¦: {folder_path} -> {candidate}; å›é€€è‡³æ ¹ç›®éŒ„")
                    record_security_event(str(folder_path), str(candidate), 'out_of_base_root')
                    self.folder_path = self.base_path
                else:
                    self.folder_path = candidate
            except Exception as e:
                logger.warning(f"[FolderScopeSecurity] è§£æè³‡æ–™å¤¾è·¯å¾‘å¤±æ•— '{folder_path}': {e}; å›é€€è‡³æ ¹ç›®éŒ„")
                record_security_event(str(folder_path), 'N/A', f'parse_error:{e}')
                self.folder_path = self.base_path
        else:
            self.folder_path = self.base_path
        self.file_cache = {}
        self.last_scan_time = 0
        self.cache_duration = 300  # 5åˆ†é˜ç·©å­˜
        # æ–‡ä»¶æ•¸é‡ä¼°ç®—èˆ‡è­¦å‘Šè³‡è¨Š
        self._file_count_warning = None
        self._file_count_warning_level = None  # high / medium / low
        self._estimated_file_count = None
        self._estimation_meta = {}
    
    def retrieve_relevant_files(self, query: str, max_files: int = 10) -> List[str]:
        """
        æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶ - å¼·åŒ–æœç´¢ç¯„åœé™åˆ¶
        """
        self._update_file_cache()
        working_file_cache = self.file_cache

        if not working_file_cache:
            logger.warning(f"åœ¨æŒ‡å®šæ–‡ä»¶å¤¾ '{self.folder_path}' ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶")
            return []

        file_count = len(working_file_cache)

        # å¼·åŒ–æ–‡ä»¶å¤¾é™åˆ¶æª¢æŸ¥
        is_folder_limited = self.folder_path != self.base_path
        if is_folder_limited:
            logger.info(f"ğŸ”’ æœç´¢ç¯„åœå·²é™åˆ¶åœ¨: {self.folder_path}")
            # é©—è­‰æ‰€æœ‰ç·©å­˜æ–‡ä»¶éƒ½åœ¨é™åˆ¶ç¯„åœå…§
            filtered_cache = {}
            folder_str = str(self.folder_path)
            for file_path, metadata in working_file_cache.items():
                if str(Path(file_path).resolve()).startswith(folder_str):
                    filtered_cache[file_path] = metadata
                else:
                    logger.debug(f"æ’é™¤ç¯„åœå¤–æ–‡ä»¶: {file_path}")
            
            working_file_cache = filtered_cache
            file_count = len(working_file_cache)
            logger.info(f"ğŸ”’ ç¯„åœé™åˆ¶å¾Œæ‰¾åˆ° {file_count} å€‹æ–‡ä»¶")

        # æª¢æŸ¥æ˜¯å¦éœ€è¦æç¤ºç”¨æˆ¶é™åˆ¶ç¯„åœ
        warning_threshold = 15000  # 1.5è¬æª”æ¡ˆä»¥ä¸Šå»ºè­°é™åˆ¶ç¯„åœ
        if not is_folder_limited and file_count > warning_threshold:
            logger.warning(f"âš ï¸  æª¢æ¸¬åˆ°å¤§é‡æ–‡ä»¶ ({file_count} å€‹)ï¼Œå¼·çƒˆå»ºè­°é¸æ“‡ç‰¹å®šè³‡æ–™å¤¾é™åˆ¶æœç´¢ç¯„åœä»¥æé«˜ç²¾åº¦")
            self._file_count_warning = f"æª¢æ¸¬åˆ°å¤§é‡æ–‡ä»¶ ({file_count} å€‹)ï¼Œå¼·çƒˆå»ºè­°é¸æ“‡ç‰¹å®šè³‡æ–™å¤¾é™åˆ¶æœç´¢ç¯„åœä»¥æé«˜æœç´¢ç²¾åº¦å’Œé€Ÿåº¦ã€‚"
            self._file_count_warning_level = "critical"

        # è‹¥é™åˆ¶ç¯„åœä¸”æ–‡ä»¶æ•¸é©ä¸­(<=3000) ç›´æ¥å…¨é‡è™•ç†ï¼Œé¿å…æ¼æª”ï¼›å¦å‰‡ä»ä½¿ç”¨é—œéµè©ç¯©é¸
        if is_folder_limited:
            if file_count <= 3000:  # æé«˜é–¾å€¼
                logger.info(f"ğŸ”’ é™åˆ¶ç¯„åœä¸‹æ‰¾åˆ° {file_count} å€‹æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ–‡ä»¶ä»¥ç¢ºä¿å¬å›ç‡ã€‚")
                return list(working_file_cache.keys())
            else:
                logger.info(f"ğŸ”’ é™åˆ¶ç¯„åœä½†æ–‡ä»¶ä»è¼ƒå¤š {file_count} å€‹ï¼Œå•Ÿç”¨æ™ºèƒ½ç¯©é¸ã€‚")

        # å…¨åŸŸæˆ–å¤§é‡æ–‡ä»¶æ™‚ç­–ç•¥
        if file_count <= 100:  # æé«˜å°æª”æ¡ˆæ•¸çš„é–¾å€¼
            logger.info(f"æ‰¾åˆ° {file_count} å€‹æ–‡ä»¶ï¼Œå°‡è™•ç†å…¨éƒ¨æ–‡ä»¶ã€‚")
            return list(working_file_cache.keys())
        
        keyword_matches = self._match_by_keywords(query, working_file_cache)
        path_matches = self._analyze_file_paths(query, working_file_cache)
        
        all_matches = list(set(keyword_matches + path_matches))
        
        scored_files = self._score_files(query, all_matches, working_file_cache)
        
        relevant_files = [file_path for file_path, score in scored_files[:max_files]]
        
        logger.info(f"åœ¨ {file_count} å€‹æ–‡ä»¶ä¸­æ‰¾åˆ° {len(relevant_files)} å€‹ç›¸é—œæ–‡ä»¶")
        return relevant_files
    
    def _quick_estimate_file_count(self, scan_path: str, max_sample_dirs: int = 50) -> Dict[str, Any]:
        """å¢å¼·ç‰ˆé›™éšæ®µå¿«é€Ÿä¼°ç®—æ–‡ä»¶æ•¸é‡ï¼Œæé«˜æ·±åº¦æƒææº–ç¢ºæ€§ã€‚

        è¿”å›: {
            'estimated_total': int,          # ä¼°ç®—ç¸½æ•¸
            'sampled_dirs': int,             # æ¡æ¨£ç›®éŒ„æ•¸
            'total_dirs_seen': int,          # èµ°è¨ªçš„ç›®éŒ„æ•¸ (å«æœªæ¡æ¨£)
            'stdev': float,
            'mean_per_dir': float,
            'ci_width': float,               # è¿‘ä¼¼ 80% å€é–“å¯¬åº¦
            'confidence': str,               # high/medium/low
            'method': str                    # 'enhanced-dual-phase' | 'fallback'
        }
        """
        result = {
            'estimated_total': 0,
            'sampled_dirs': 0,
            'total_dirs_seen': 0,
            'stdev': 0.0,
            'mean_per_dir': 0.0,
            'ci_width': 0.0,
            'confidence': 'low',
            'method': 'enhanced-dual-phase'
        }
        try:
            import statistics
            sample_counts = []
            depth_adjusted = []
            total_dirs = 0
            sampled = 0
            max_depth = 8  # å¢åŠ æƒææ·±åº¦å¾4åˆ°8
            deep_dirs_sampled = 0  # æ·±å±¤ç›®éŒ„æ¡æ¨£è¨ˆæ•¸

            # ç¬¬ä¸€éšæ®µï¼šå»£åº¦å„ªå…ˆæƒæ
            for root, dirs, files in os.walk(scan_path):
                depth = root.replace(scan_path, '').count(os.sep)
                if depth > max_depth:
                    dirs[:] = []
                    continue
                total_dirs += 1

                # éæ¿¾ç³»çµ±ç›®éŒ„
                dirs[:] = [d for d in dirs if not d.startswith('.') and d.lower() not in ['system','temp','tmp','$recycle.bin','windows.old','recovery']]

                # æ ¹æ“šæ·±åº¦èª¿æ•´æ¡æ¨£ç­–ç•¥
                should_sample = False
                if depth <= 2:
                    # æ·ºå±¤ç›®éŒ„ï¼šé«˜æ¡æ¨£ç‡
                    should_sample = sampled < max_sample_dirs * 0.6
                elif depth <= 5:
                    # ä¸­å±¤ç›®éŒ„ï¼šä¸­ç­‰æ¡æ¨£ç‡
                    should_sample = sampled < max_sample_dirs * 0.8 and total_dirs % 2 == 0
                else:
                    # æ·±å±¤ç›®éŒ„ï¼šç¢ºä¿æœ‰è¶³å¤ æ·±å±¤æ¡æ¨£
                    should_sample = (deep_dirs_sampled < max_sample_dirs * 0.3 and 
                                   total_dirs % 3 == 0 and sampled < max_sample_dirs)

                if should_sample:
                    supported_files = sum(1 for f in files if os.path.splitext(f)[1].lower() in SUPPORTED_FILE_TYPES)
                    sample_counts.append(supported_files)
                    
                    # æ ¹æ“šæ·±åº¦èª¿æ•´æ¬Šé‡
                    if depth <= 1:
                        depth_adjusted.append(supported_files * 1.2)  # æ ¹ç›®éŒ„æ¬Šé‡ç¨é«˜
                    elif depth <= 3:
                        depth_adjusted.append(supported_files * 1.0)  # æ¨™æº–æ¬Šé‡
                    elif depth <= 6:
                        depth_adjusted.append(supported_files * 0.95)  # ç¨ä½æ¬Šé‡
                    else:
                        depth_adjusted.append(supported_files * 0.9)   # æ·±å±¤æ¬Šé‡
                    
                    sampled += 1
                    if depth > 5:
                        deep_dirs_sampled += 1

            if sampled == 0 or total_dirs == 0:
                return result

            mean_raw = sum(sample_counts) / sampled
            mean_adj = sum(depth_adjusted) / sampled
            try:
                stdev = statistics.pstdev(sample_counts)
            except Exception:
                stdev = 0.0
            ci_width = 1.28 * stdev

            # åŸºæ–¼æ·±åº¦åˆ†ä½ˆçš„ä¼°ç®—ä¿®æ­£
            estimated_total = int(mean_adj * total_dirs)
            
            # å¦‚æœæ·±å±¤ç›®éŒ„æ¡æ¨£ä¸è¶³ï¼Œæ‡‰ç”¨ä¿å®ˆä¿®æ­£ä¿‚æ•¸
            if deep_dirs_sampled < max_sample_dirs * 0.1 and total_dirs > 100:
                estimated_total = int(estimated_total * 1.15)  # å¢åŠ 15%ä¿å®ˆä¼°è¨ˆ
                result['method'] = 'enhanced-dual-phase-corrected'

            # å¤§è®Šç•°ä¿å®ˆä¿®æ­£
            if stdev > mean_raw * 1.5 and estimated_total > mean_raw * total_dirs * 1.4:
                estimated_total = int(mean_raw * total_dirs * 1.3)

            # ç¬¬äºŒéšæ®µï¼šé‡å°ä¼°ç®—æ¥è¿‘è‡¨ç•Œå€¼æ™‚çš„è£œå……æƒæ
            high_band_lower = 8000   # é™ä½ä¸‹é™
            high_band_upper = 15000  # æé«˜ä¸Šé™
            if high_band_lower <= estimated_total <= high_band_upper and sampled < total_dirs:
                logger.info(f"ä¼°ç®—å€¼ {estimated_total} æ¥è¿‘è‡¨ç•Œå€¼ï¼Œå•Ÿå‹•è£œå……æ·±åº¦æƒæ")
                refine_dirs = 0
                refine_limit = 20  # å¢åŠ è£œå……æƒææ•¸é‡
                additional_samples = []
                
                # é‡æ–°èµ°è¨ªï¼Œé‡é»æ¡æ¨£ä¹‹å‰æ¼æ‰çš„æ·±å±¤ç›®éŒ„
                for root, dirs, files in os.walk(scan_path):
                    if refine_dirs >= refine_limit:
                        break
                    depth = root.replace(scan_path, '').count(os.sep)
                    if depth > max_depth or depth < 3:  # é‡é»æƒæä¸­æ·±å±¤
                        dirs[:] = []
                        continue
                    
                    # åªæ¡æ¨£ä¹‹å‰æ²’æœ‰æ¡æ¨£éçš„å€åŸŸ (ç°¡åŒ–åˆ¤æ–·)
                    if refine_dirs < refine_limit:
                        supported_files = sum(1 for f in files if os.path.splitext(f)[1].lower() in SUPPORTED_FILE_TYPES)
                        additional_samples.append(supported_files)
                        refine_dirs += 1

                if additional_samples:
                    # åˆä½µæ¨£æœ¬é‡æ–°è¨ˆç®—
                    all_samples = sample_counts + additional_samples
                    sampled_total = len(all_samples)
                    mean_raw = sum(all_samples) / sampled_total
                    try:
                        stdev = statistics.pstdev(all_samples)
                    except Exception:
                        stdev = 0.0
                    ci_width = 1.28 * stdev
                    estimated_total = int(mean_raw * total_dirs)
                    result['method'] = 'enhanced-dual-phase-refined'

            # ä¿¡å¿ƒè©•ä¼°ï¼šç›¸å°èª¤å·® proxy = ci_width / (mean_raw + 1)
            rel_ci_ratio = ci_width / (mean_raw + 1)
            if rel_ci_ratio < 0.4 and sampled >= 20 and deep_dirs_sampled >= 5:
                confidence = 'high'
            elif rel_ci_ratio < 0.8 and sampled >= 10:
                confidence = 'medium'
            else:
                confidence = 'low'

            result.update({
                'estimated_total': estimated_total,
                'sampled_dirs': sampled,
                'total_dirs_seen': total_dirs,
                'stdev': round(stdev, 2),
                'mean_per_dir': round(mean_raw, 2),
                'ci_width': round(ci_width, 2),
                'confidence': confidence,
                'deep_dirs_sampled': deep_dirs_sampled
            })

            logger.info(
                f"æ–‡ä»¶æ•¸ä¼°ç®—(å¢å¼·ç‰ˆ): est={estimated_total} dirs={sampled}/{total_dirs} deep={deep_dirs_sampled} mean={mean_raw:.2f} Ïƒ={stdev:.2f} ciâ‰ˆÂ±{ci_width:.1f} conf={confidence} method={result['method']}"
            )
            return result
        except Exception as e:
            logger.error(f"å¢å¼·ç‰ˆå¿«é€Ÿä¼°ç®—æ–‡ä»¶æ•¸é‡å¤±æ•—: {str(e)}")
            result['method'] = 'error'
            return result

    def _update_file_cache(self):
        """æ›´æ–°æ–‡ä»¶ç·©å­˜ - æ€§èƒ½å„ªåŒ–ç‰ˆæœ¬"""
        current_time = time.time()
        if current_time - self.last_scan_time < self.cache_duration:
            return
        
        # ç¢ºå®šæƒæè·¯å¾‘
        scan_path = str(self.folder_path)
        logger.info(f"æ›´æ–°æ–‡ä»¶ç·©å­˜ï¼Œæƒæè·¯å¾‘: {scan_path}")
        # å¿«é€Ÿä¼°ç®—æ–‡ä»¶æ•¸é‡ (å«ä¿¡å¿ƒèˆ‡æ–¹æ³•)
        estimation = self._quick_estimate_file_count(scan_path)
        estimated_count = estimation.get('estimated_total', 0)
        self._estimated_file_count = estimated_count
        self._estimation_meta = estimation  # ä¿å­˜è©³ç´°å…ƒæ•¸æ“šä¾› scope_info ä½¿ç”¨
        
        # --- è‡ªé©æ‡‰é–¾å€¼æ ¡æº– ---
        # è®€å–æœ€è¿‘ N æ¢ estimation_audit.log è¨ˆç®— MAE% / åå·®ï¼Œå‹•æ…‹èª¿æ•´åˆ†æ®µé–¾å€¼
        high_cut = 60000   # æé«˜åˆ°6è¬ï¼Œå°æ‡‰ç”¨æˆ¶å¯¦éš›çš„æª”æ¡ˆæ•¸é‡
        fast_cut = 40000   # 4è¬ä»¥ä¸Šä½¿ç”¨å¿«é€Ÿæƒæ
        medium_cut = 20000 # 2è¬ä»¥ä¸Šä½¿ç”¨ä¸­ç­‰æƒæ
        low_cut = 10000    # 1è¬ä»¥ä¸Šçµ¦å‡ºæç¤º
        try:
            from config.config import LOGS_DIR
            audit_path = os.path.join(LOGS_DIR, 'estimation_audit.log')
            if os.path.exists(audit_path):
                with open(audit_path, 'r', encoding='utf-8') as f:
                    recent = f.readlines()[-80:]  # æœ€è¿‘ 80 æ¢
                errs = []
                for line in recent:
                    # err_pct=XX.X
                    if 'err_pct=' in line:
                        try:
                            part = line.split('err_pct=')[1].split()[0]
                            val = float(part)
                            if abs(val) < 500:  # éæ¿¾ç•°å¸¸å€¼
                                errs.append(val)
                        except Exception:
                            continue
                if len(errs) >= 8:
                    import statistics
                    mae = statistics.mean(abs(e) for e in errs)
                    bias = statistics.mean(errs)
                    # è‹¥ MAE æ¥µä½ (<15%) ä¸”åå·®ä¸è¶…é +-10%ï¼Œæ”¾å¯¬é–¾å€¼ 8-12% ä»¥æ¸›å°‘ä¸å¿…è¦çš„é«˜é¢¨éšªæ¨™è¨˜
                    if mae < 15 and abs(bias) < 10:
                        medium_cut = int(medium_cut * 1.08)
                        fast_cut = int(fast_cut * 1.1)
                        high_cut = int(high_cut * 1.12)
                    # è‹¥ MAE éé«˜ (>40%) æˆ– åå·®å¹…åº¦å¤§ (>30%)ï¼Œæ”¶ç·Šé–¾å€¼ 5-10% ä»¥æ›´ä¿å®ˆ
                    elif mae > 40 or abs(bias) > 30:
                        medium_cut = int(medium_cut * 0.93)
                        fast_cut = int(fast_cut * 0.9)
                        high_cut = int(high_cut * 0.88)
                    logger.debug(f"[AdaptiveThreshold] mae={mae:.1f}% bias={bias:.1f}% -> cuts medium={medium_cut} fast={fast_cut} high={high_cut}")
        except Exception:
            pass

        # æ ¹æ“šï¼ˆå¯èƒ½èª¿æ•´å¾Œçš„ï¼‰ä¼°ç®—çµæœæ±ºå®šæƒæç­–ç•¥
        if estimated_count > high_cut:
            logger.warning(f"æª¢æ¸¬åˆ°æ¥µå¤§é‡æ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œå»ºè­°ç¸®å°æœç´¢ç¯„åœä»¥æé«˜æ€§èƒ½")
            self._file_count_warning = f"æª¢æ¸¬åˆ°æ¥µå¤§é‡æ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œå»ºè­°ç¸®å°æœç´¢ç¯„åœä»¥ç²å¾—æ›´å¥½çš„æœç´¢æ•ˆæœã€‚"
            self._file_count_warning_level = "critical"
            max_files = 2000
            max_depth = 4
        elif estimated_count > fast_cut:
            logger.warning(f"æª¢æ¸¬åˆ°å¤§é‡æ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œä½¿ç”¨å¿«é€Ÿæƒææ¨¡å¼")
            self._file_count_warning = f"æª¢æ¸¬åˆ°å¤§é‡æ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œå»ºè­°é¸æ“‡ç‰¹å®šè³‡æ–™å¤¾ç¯„åœä»¥æé«˜æœç´¢ç²¾åº¦ã€‚"
            self._file_count_warning_level = "high"
            max_files = 3000
            max_depth = 5
        elif estimated_count > medium_cut:
            logger.info(f"æª¢æ¸¬åˆ°è¼ƒå¤šæ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œä½¿ç”¨ä¸­ç­‰æƒææ¨¡å¼")
            self._file_count_warning = f"æª¢æ¸¬åˆ°è¼ƒå¤šæ–‡ä»¶ (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œè‹¥æœç´¢çµæœä¸ç†æƒ³å¯è€ƒæ…®é™åˆ¶æœç´¢ç¯„åœã€‚"
            self._file_count_warning_level = "medium"
            max_files = 5000
            max_depth = 6
        elif estimated_count > low_cut:
            logger.info(f"æª¢æ¸¬åˆ°æ–‡ä»¶æ•¸é‡è¼ƒå¤š (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œå»ºè­°è€ƒæ…®é™åˆ¶æœç´¢ç¯„åœ")
            self._file_count_warning = f"æª¢æ¸¬åˆ°æ–‡ä»¶æ•¸é‡è¼ƒå¤š (ä¼°ç®—ç´„ {estimated_count} å€‹)ï¼Œå¯è€ƒæ…®é™åˆ¶æœç´¢ç¯„åœä»¥ç²å¾—æ›´ç²¾ç¢ºçš„çµæœã€‚"
            self._file_count_warning_level = "low"
            max_files = 8000
            max_depth = 7
        else:
            logger.info(f"æ–‡ä»¶æ•¸é‡é©ä¸­ ({estimated_count} å€‹)ï¼Œä½¿ç”¨å®Œæ•´æƒææ¨¡å¼")
            self._file_count_warning = None
            self._file_count_warning_level = "none"
            max_files = 10000
            max_depth = 8
        
        self.file_cache = {}
        
        try:
            file_count = 0  # ä¿ç•™è¨ˆæ•¸å™¨ä»¥ä¾¿æœªä¾†æ“´å±•
            # ä½¿ç”¨æ›´é«˜æ•ˆçš„æƒæç­–ç•¥
            self._scan_directories_efficiently(scan_path, max_files, max_depth)
            actual_count = len(self.file_cache)
            self.last_scan_time = current_time
            # æ˜¯å¦é”åˆ°ä¸Šé™ï¼ˆå¯èƒ½æˆªæ–·ï¼‰
            truncated = actual_count >= max_files and estimated_count > actual_count
            self._scan_truncated = truncated
            self._actual_file_cache_size = actual_count
            logger.info(f"æ–‡ä»¶ç·©å­˜æ›´æ–°å®Œæˆï¼Œå…± {actual_count} å€‹æ–‡ä»¶ (ä¼°ç®— {estimated_count}ï¼Œæˆªæ–·={truncated})")
            # å¯«å…¥ä¼°ç®—å¯©è¨ˆæ—¥èªŒï¼ˆé™„å¸¶èª¤å·®ç™¾åˆ†æ¯”ï¼‰
            try:
                from config.config import LOGS_DIR
                os.makedirs(LOGS_DIR, exist_ok=True)
                audit_path = os.path.join(LOGS_DIR, 'estimation_audit.log')
                err_pct = None
                if estimated_count and actual_count:
                    err_pct = round((estimated_count - actual_count) / max(actual_count, 1) * 100, 2)
                with open(audit_path, 'a', encoding='utf-8') as f:
                    f.write(f"ts={int(current_time)} path={scan_path} est={estimated_count} actual={actual_count} err_pct={err_pct} conf={estimation.get('confidence')} sampled={estimation.get('sampled_dirs')} total_dirs={estimation.get('total_dirs_seen')} method={estimation.get('method')} truncated={truncated}\n")
            except Exception:
                pass
        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡ä»¶ç·©å­˜å¤±æ•—: {str(e)}")
            # å¦‚æœæƒæå¤±æ•—ï¼Œè‡³å°‘å˜—è©¦æƒææ ¹ç›®éŒ„çš„å‰50å€‹æ–‡ä»¶
            self._fallback_scan(scan_path)

    def _scan_directories_efficiently(self, scan_path: str, max_files: int, max_depth: int):
        """é«˜æ•ˆæƒæç›®éŒ„ - ä½¿ç”¨æ‰¹é‡è™•ç†å’Œå„ªåŒ–ç­–ç•¥"""
        file_count = 0
        
        try:
            # ä½¿ç”¨ç”Ÿæˆå™¨é¿å…ä¸€æ¬¡æ€§åŠ è¼‰æ‰€æœ‰æ–‡ä»¶
            for root, dirs, files in os.walk(scan_path):
                # è¨ˆç®—ç•¶å‰æ·±åº¦
                depth = root.replace(scan_path, '').count(os.sep)
                    
                if depth >= max_depth:
                    dirs[:] = []  # ä¸å†æ·±å…¥å­ç›®éŒ„
                    continue
                
                # è·³éç³»çµ±ç›®éŒ„å’Œéš±è—ç›®éŒ„
                dirs[:] = [d for d in dirs if not d.startswith('.') and d.lower() not in ['system', 'temp', 'tmp', '$recycle.bin']]
                
                # æ‰¹é‡è™•ç†æ–‡ä»¶ï¼Œé¿å…é€å€‹è™•ç†
                supported_files = [f for f in files[:300] if os.path.splitext(f)[1].lower() in SUPPORTED_FILE_TYPES 
                                 and not f.startswith('.') and not f.startswith('~') and not f.lower().endswith('.tmp')]
                
                # ä½¿ç”¨æ‰¹é‡statæ“ä½œ
                for file in supported_files:
                    if file_count >= max_files:
                        logger.info(f"é”åˆ°æ–‡ä»¶æ•¸é‡é™åˆ¶ ({max_files})ï¼Œåœæ­¢æƒæ")
                        return
                    
                    file_path = os.path.join(root, file)
                    
                    try:
                        # ä½¿ç”¨æ›´å¿«çš„æ–‡ä»¶å¤§å°æª¢æŸ¥
                        file_size = os.path.getsize(file_path)
                        if file_size < 100:  # è·³ééå°çš„æ–‡ä»¶
                            continue
                        
                        # åªåœ¨éœ€è¦æ™‚ç²å–å®Œæ•´statä¿¡æ¯
                        stat_info = os.stat(file_path)
                        
                        # è¨ˆç®—ç›¸å°è·¯å¾‘
                        display_path = os.path.relpath(file_path, self.base_path).replace('\\', '/')
                        
                        self.file_cache[file_path] = {
                            'name': file,
                            'size': file_size,
                            'mtime': stat_info.st_mtime,
                            'ext': os.path.splitext(file)[1].lower(),
                            'relative_path': display_path,
                            'depth': depth,
                            'folder_limited': self.folder_path != self.base_path
                        }
                        file_count += 1
                        
                    except (OSError, PermissionError):
                        continue
                
                # æ¯è™•ç†1000å€‹æ–‡ä»¶è¨˜éŒ„ä¸€æ¬¡é€²åº¦
                if file_count > 0 and file_count % 1000 == 0:
                    logger.info(f"å·²æƒæ {file_count} å€‹æ–‡ä»¶...")
                    
        except Exception as e:
            logger.error(f"é«˜æ•ˆæƒæå¤±æ•—: {str(e)}")

    def _fallback_scan(self, scan_path: str):
        """å›é€€æƒææ–¹æ³• - åªæƒææ ¹ç›®éŒ„"""
        try:
            logger.info("ä½¿ç”¨å›é€€æƒææ–¹æ³•...")
            files = os.listdir(scan_path)[:100]  # åªæƒæå‰100å€‹é …ç›®
            
            for file in files:
                file_path = os.path.join(scan_path, file)
                if os.path.isfile(file_path):
                    file_ext = os.path.splitext(file)[1].lower()
                    if file_ext in SUPPORTED_FILE_TYPES:
                        try:
                            stat_info = os.stat(file_path)
                            if stat_info.st_size >= 100:  # è·³ééå°çš„æ–‡ä»¶
                                self.file_cache[file_path] = {
                                    'name': file,
                                    'size': stat_info.st_size,
                                    'mtime': stat_info.st_mtime,
                                    'ext': file_ext,
                                    'relative_path': file,
                                    'depth': 0,
                                    'folder_limited': self.folder_path != self.base_path
                                }
                        except (OSError, PermissionError):
                            continue
            
            logger.info(f"å›é€€æƒæå®Œæˆï¼Œæ‰¾åˆ° {len(self.file_cache)} å€‹æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"å›é€€æƒæä¹Ÿå¤±æ•—: {str(e)}")
            self.file_cache = {}
    
    def _match_by_keywords(self, query: str, file_cache: dict) -> List[str]:
        """åŸºæ–¼é—œéµè©åŒ¹é…æ–‡ä»¶"""
        query_lower = query.lower()
        query_words = query_lower.split()
        
        matches = []
        for file_path, metadata in file_cache.items():
            file_name_lower = metadata['name'].lower()
            path_lower = metadata['relative_path'].lower()
            
            # è¨ˆç®—åŒ¹é…åˆ†æ•¸
            score = 0
            for word in query_words:
                # å®Œæ•´è©åŒ¹é…
                if word in file_name_lower:
                    score += 2  # æ–‡ä»¶ååŒ¹é…æ¬Šé‡æ›´é«˜
                if word in path_lower:
                    score += 1  # è·¯å¾‘åŒ¹é…
                
                # éƒ¨åˆ†è©åŒ¹é…ï¼ˆæ›´å¯¬é¬†çš„åŒ¹é…ï¼‰
                for file_word in file_name_lower.split():
                    if word in file_word or file_word in word:
                        score += 1
            
            # å¦‚æœæ²’æœ‰é—œéµè©åŒ¹é…ï¼Œä½†æŸ¥è©¢å¾ˆçŸ­ï¼Œå‰‡åŒ…å«æ‰€æœ‰æ–‡ä»¶
            if score == 0 and len(query_words) <= 2:
                score = 0.1  # çµ¦ä¸€å€‹å¾ˆå°çš„åˆ†æ•¸
            
            if score > 0:
                matches.append((file_path, score))
        
        # æŒ‰åˆ†æ•¸æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return [file_path for file_path, score in matches]
    
    def _analyze_file_paths(self, query: str, file_cache: dict) -> List[str]:
        """åŸºæ–¼è·¯å¾‘èªç¾©åˆ†æåŒ¹é…æ–‡ä»¶"""
        # ç°¡åŒ–ç‰ˆæœ¬ï¼šåŸºæ–¼è·¯å¾‘é—œéµè©
        path_keywords = {
            'æ”¿ç­–': ['policy', 'regulation', 'æ”¿ç­–', 'è¦å®š'],
            'æµç¨‹': ['process', 'procedure', 'æµç¨‹', 'ç¨‹åº'],
            'æ‰‹å†Š': ['manual', 'handbook', 'æ‰‹å†Š', 'æŒ‡å—'],
            'å ±å‘Š': ['report', 'å ±å‘Š', 'åˆ†æ'],
            'åˆç´„': ['contract', 'agreement', 'åˆç´„', 'å”è­°']
        }
        
        matches = []
        query_lower = query.lower()
        
        for category, keywords in path_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                for file_path, metadata in file_cache.items():
                    path_lower = metadata['relative_path'].lower()
                    if any(keyword in path_lower for keyword in keywords):
                        matches.append(file_path)
        
        return matches
    
    def _score_files(self, query: str, file_paths: List[str], file_cache: dict) -> List[Tuple[str, float]]:
        """ç‚ºæ–‡ä»¶è©•åˆ†ä¸¦æ’åº"""
        scored_files = []
        
        for file_path in file_paths:
            if file_path not in file_cache:
                continue
                
            metadata = file_cache[file_path]
            score = 0
            
            # æ–‡ä»¶åç›¸é—œæ€§
            file_name_lower = metadata['name'].lower()
            query_lower = query.lower()
            
            # é—œéµè©åŒ¹é…åˆ†æ•¸
            query_words = query_lower.split()
            for word in query_words:
                if word in file_name_lower:
                    score += 2
                if word in metadata['relative_path'].lower():
                    score += 1
            
            # æ–‡ä»¶é¡å‹æ¬Šé‡
            type_weights = {
                '.pdf': 1.2,
                '.docx': 1.1,
                '.txt': 1.0,
                '.md': 1.0,
                '.xlsx': 0.9,
                '.pptx': 0.8
            }
            score *= type_weights.get(metadata['ext'], 1.0)
            
            # æ–‡ä»¶å¤§å°æ¬Šé‡ï¼ˆé©ä¸­å¤§å°çš„æ–‡ä»¶å¯èƒ½åŒ…å«æ›´å¤šæœ‰ç”¨ä¿¡æ¯ï¼‰
            size_kb = metadata['size'] / 1024
            if 10 <= size_kb <= 1000:  # 10KB - 1MB
                score *= 1.1
            elif size_kb > 5000:  # å¤§æ–¼5MBçš„æ–‡ä»¶é™æ¬Š
                score *= 0.8
            
            # æœ€è¿‘ä¿®æ”¹æ™‚é–“æ¬Šé‡
            days_old = (time.time() - metadata['mtime']) / (24 * 3600)
            if days_old < 30:  # 30å¤©å…§çš„æ–‡ä»¶
                score *= 1.1
            elif days_old > 365:  # è¶…éä¸€å¹´çš„æ–‡ä»¶
                score *= 0.9
            
            scored_files.append((file_path, score))
        
        # æŒ‰åˆ†æ•¸æ’åº
        scored_files.sort(key=lambda x: x[1], reverse=True)
        return scored_files



class DynamicContentProcessor:
    """å‹•æ…‹å…§å®¹è™•ç†å™¨ - å³æ™‚è§£æå’Œè™•ç†æ–‡ä»¶å…§å®¹"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=MAX_TOKENS_CHUNK,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
        )
        self.content_cache = {}  # å…§å®¹ç·©å­˜
        self.cache_duration = 600  # 10åˆ†é˜ç·©å­˜
    
    def process_files(self, file_paths: List[str]) -> List[Document]:
        """
        ä¸¦è¡Œè™•ç†å¤šå€‹æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
            
        Returns:
            è™•ç†å¾Œçš„æ–‡æª”åˆ—è¡¨
        """
        documents = []
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_file = {
                executor.submit(self._process_single_file, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_documents = future.result()
                    documents.extend(file_documents)
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ä»¶ {file_path} å¤±æ•—: {str(e)}")
        
        return documents
    
    def _process_single_file(self, file_path: str) -> List[Document]:
        """è™•ç†å–®å€‹æ–‡ä»¶"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = self._get_cache_key(file_path)
        if cache_key in self.content_cache:
            cache_entry = self.content_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_duration:
                return cache_entry['documents']
        
        try:
            # ç²å–è§£æå™¨
            parser = FileParser.get_parser_for_file(file_path)
            if not parser:
                logger.warning(f"ç„¡æ³•ç²å–æ–‡ä»¶è§£æå™¨: {file_path}")
                return []
            
            # è§£ææ–‡ä»¶å…§å®¹
            content_blocks = parser.safe_parse(file_path)
            if not content_blocks:
                return []
            
            # å‰µå»ºæ–‡æª”
            documents = []
            for text, metadata in content_blocks:
                if not text or not text.strip():
                    continue
                
                # åˆ†å‰²æ–‡æœ¬
                chunks = self.text_splitter.split_text(text)
                
                for i, chunk in enumerate(chunks):
                    chunk_metadata = metadata.copy()
                    chunk_metadata.update({
                        "chunk_id": i,
                        "chunk_count": len(chunks),
                        "file_path": file_path
                    })
                    
                    doc = Document(
                        page_content=chunk,
                        metadata=chunk_metadata
                    )
                    documents.append(doc)
            
            # ç·©å­˜çµæœ
            self.content_cache[cache_key] = {
                'documents': documents,
                'timestamp': time.time()
            }
            
            return documents
            
        except Exception as e:
            logger.error(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {str(e)}")
            return []
    
    def _get_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        try:
            stat = os.stat(file_path)
            content = f"{file_path}_{stat.st_mtime}_{stat.st_size}"
            return hashlib.md5(content.encode()).hexdigest()
        except OSError:
            return hashlib.md5(file_path.encode()).hexdigest()


class RealTimeVectorizer:
    """å³æ™‚å‘é‡åŒ–å¼•æ“"""
    
    def __init__(self, embedding_model: str, platform: str = "ollama"):
        # æ ¹æ“šå¹³å°é¸æ“‡åµŒå…¥æ¨¡å‹
        if platform == "ollama":
            from utils.ollama_embeddings import OllamaEmbeddings
            self.embeddings = OllamaEmbeddings(model_name=embedding_model)
            logger.info(f"ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹: {embedding_model}")
        else: # é»˜èªç‚º huggingface
            self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
            logger.info(f"ä½¿ç”¨ Hugging Face åµŒå…¥æ¨¡å‹: {embedding_model}")
        
        self.query_cache = {}  # æŸ¥è©¢å‘é‡ç·©å­˜
        self.cache_duration = 1800  # 30åˆ†é˜ç·©å­˜
    
    def vectorize_query(self, query: str) -> np.ndarray:
        """å‘é‡åŒ–æŸ¥è©¢"""
        # æª¢æŸ¥ç·©å­˜
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.query_cache:
            cache_entry = self.query_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_duration:
                return cache_entry['vector']
        
        try:
            vector = self.embeddings.embed_query(query)
            vector_array = np.array(vector)
            
            # ç·©å­˜çµæœ
            self.query_cache[cache_key] = {
                'vector': vector_array,
                'timestamp': time.time()
            }
            
            return vector_array
        except Exception as e:
            logger.error(f"æŸ¥è©¢å‘é‡åŒ–å¤±æ•—: {str(e)}")
            return np.array([])
    
    def vectorize_documents(self, documents: List[Document]) -> List[Tuple[Document, np.ndarray]]:
        """å‘é‡åŒ–æ–‡æª”"""
        doc_vectors = []
        
        try:
            # æ‰¹é‡å‘é‡åŒ–
            texts = [doc.page_content for doc in documents]
            vectors = self.embeddings.embed_documents(texts)
            
            for doc, vector in zip(documents, vectors):
                doc_vectors.append((doc, np.array(vector)))
                
        except Exception as e:
            logger.error(f"æ–‡æª”å‘é‡åŒ–å¤±æ•—: {str(e)}")
        
        return doc_vectors
    
    def calculate_similarities(self, query_vector: np.ndarray, 
                             doc_vectors: List[Tuple[Document, np.ndarray]]) -> List[Tuple[Document, float]]:
        """è¨ˆç®—ç›¸ä¼¼åº¦"""
        if len(query_vector) == 0:
            return []
        
        similarities = []
        
        for doc, doc_vector in doc_vectors:
            if len(doc_vector) == 0:
                continue
                
            try:
                # æ‰‹å‹•è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(query_vector, doc_vector)
                similarities.append((doc, similarity))
            except Exception as e:
                logger.error(f"è¨ˆç®—ç›¸ä¼¼åº¦å¤±æ•—: {str(e)}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        æ‰‹å‹•è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: ç¬¬ä¸€å€‹å‘é‡
            vec2: ç¬¬äºŒå€‹å‘é‡
            
        Returns:
            é¤˜å¼¦ç›¸ä¼¼åº¦å€¼
        """
        try:
            # è¨ˆç®—é»ç©
            dot_product = np.dot(vec1, vec2)
            
            # è¨ˆç®—å‘é‡çš„æ¨¡é•·
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            # é¿å…é™¤é›¶éŒ¯èª¤
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            similarity = dot_product / (norm1 * norm2)
            
            # ç¢ºä¿çµæœåœ¨ [-1, 1] ç¯„åœå…§
            return float(np.clip(similarity, -1.0, 1.0))
            
        except Exception as e:
            logger.error(f"è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {str(e)}")
            return 0.0


class DynamicRAGEngineBase(RAGEngineInterface):
    """å‹•æ…‹RAGå¼•æ“åŸºåº•é¡åˆ¥ - åŒ…å«å…±é€šé‚è¼¯"""

    REWRITE_PROMPT_TEMPLATE = ""
    ANSWER_PROMPT_TEMPLATE = ""
    RELEVANCE_PROMPT_TEMPLATE = ""
    BATCH_RELEVANCE_PROMPT_TEMPLATE = "" # New template

    def __init__(self, ollama_model: str, ollama_embedding_model: str, platform: str = "ollama", folder_path: Optional[str] = None):
        self.ollama_model = ollama_model
        self.ollama_embedding_model = ollama_embedding_model
        self.platform = platform
        self.folder_path = folder_path
        
        # åˆå§‹åŒ–çµ„ä»¶ï¼Œå‚³éæ–‡ä»¶å¤¾è·¯å¾‘
        self.file_retriever = SmartFileRetriever(folder_path=folder_path)
        self.content_processor = DynamicContentProcessor()
        self.vectorizer = RealTimeVectorizer(ollama_embedding_model, platform=self.platform)
        
        # åˆå§‹åŒ–èªè¨€æ¨¡å‹
        if self.platform == "ollama":
            # èˆ‡å‚³çµ± English RAG å°é½Šæº«åº¦ (0.4) ä»¥æå‡å›ç­”éˆæ´»åº¦
            llm_params = {
                "temperature": 0.4,
                "timeout": OLLAMA_REQUEST_TIMEOUT,
                "request_timeout": OLLAMA_REQUEST_TIMEOUT
            }
        else:  # Hugging Face
            llm_params = {
                "temperature": 0.1,
                "max_new_tokens": 1024,
                "top_p": 0.9,
                "top_k": 50,
                "repetition_penalty": 1.15  # ç¨å¾®å¢åŠ é‡è¤‡æ‡²ç½°
            }

        # é€šç”¨ç”Ÿæˆåƒæ•¸å¾®èª¿ï¼šçµ±ä¸€çš„ã€Œç²¾ç°¡ã€é¿å…é‡è¤‡ã€æ§åˆ¶é•·åº¦ã€ç­–ç•¥ï¼ˆç§»é™¤æ¨¡å‹ç‰¹å®šåˆ¤æ–·ï¼‰
        # ç›®çš„ï¼š
        # 1. é¿å…å€‹åˆ¥æ¨¡å‹ç‰¹åŒ–é€ æˆç¶­è­·è² æ“”
        # 2. ç‚ºæ‰€æœ‰æ¨¡å‹æä¾›ä¸€è‡´çš„ç°¡æ½”è¼¸å‡ºå‚¾å‘
        # 3. æ§åˆ¶æœ€å¤§ç”Ÿæˆé•·åº¦æ¸›å°‘ç„¡é—œè´…è¿°èˆ‡æˆæœ¬
        try:
            if self.platform == "ollama":
                # è‹¥æœªæŒ‡å®šï¼Œçµ¦å‡ºä¿å®ˆä¸Šé™èˆ‡é©åº¦å¤šæ¨£æ€§
                llm_params.setdefault("num_predict", 800)  # é€šç”¨ä¸Šé™
                llm_params.setdefault("top_p", 0.9)
                llm_params.setdefault("repeat_penalty", 1.12)
            else:  # Hugging Face / transformers æ¨ç†
                llm_params.setdefault("max_new_tokens", 768)  # é©åº¦é™åˆ¶
                llm_params.setdefault("repetition_penalty", 1.18)
                llm_params.setdefault("top_p", 0.9)
                # é©ä¸­æº«åº¦ï¼šå…¼é¡§è¦†è¿°èˆ‡å¤šæ¨£æ€§
                if "temperature" in llm_params and llm_params["temperature"] < 0.15:
                    llm_params["temperature"] = 0.3
                else:
                    llm_params.setdefault("temperature", 0.3)
        except Exception:
            pass

        try:
            if self.platform == "ollama":
                from langchain_ollama import OllamaLLM
                from config.config import OLLAMA_HOST
                self.llm = OllamaLLM(
                    model=ollama_model,
                    base_url=OLLAMA_HOST,
                    **llm_params
                )
                logger.info(f"ä½¿ç”¨ Ollama èªè¨€æ¨¡å‹: {ollama_model}")
            else:  # é»˜èªç‚º huggingface
                self.llm = ChatHuggingFace(
                    model_name=ollama_model,
                    **llm_params
                )
                logger.info(f"ä½¿ç”¨ Hugging Face èªè¨€æ¨¡å‹ (é€é ModelManager): {ollama_model} with params: {llm_params}")
        except Exception as e:
            logger.error(f"èªè¨€æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            logger.info("å›é€€åˆ°ä½¿ç”¨ ChatHuggingFace ä½œç‚ºæœ€çµ‚æ–¹æ¡ˆ")
            self.llm = ChatHuggingFace(
                model_name=ollama_model,
                **llm_params
            )

        # é¡¯å¼è¨˜éŒ„æœ€çµ‚èªè¨€èˆ‡å­é¡
        try:
            lang_info = self.get_language()
        except Exception:
            lang_info = "æœªçŸ¥"
        logger.info(f"Dynamic RAG Engine åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {ollama_model}ï¼Œèªè¨€: {lang_info}ï¼Œå¼•æ“: {self.__class__.__name__}")

    def rewrite_query(self, original_query: str) -> str:
        """æŸ¥è©¢é‡å¯« - åƒç…§å‚³çµ±RAGçš„é‡è©¦æ©Ÿåˆ¶å’Œç­–ç•¥"""
        try:
            if len(original_query.strip()) <= 3:
                return original_query
            
            # åƒç…§å‚³çµ±RAGçš„é‡è©¦æ©Ÿåˆ¶å’Œé…ç½®
            from config.config import (
                OLLAMA_MAX_RETRIES, OLLAMA_RETRY_DELAY, 
                OLLAMA_QUERY_OPTIMIZATION_TIMEOUT
            )
            import time
            import concurrent.futures
            
            for attempt in range(OLLAMA_MAX_RETRIES):
                try:
                    # ä½¿ç”¨èˆ‡å‚³çµ±RAGç›¸åŒçš„å„ªåŒ–ç­–ç•¥å’Œæ¨¡æ¿æ ¼å¼
                    prompt = self.REWRITE_PROMPT_TEMPLATE.format(original_query=original_query)
                    
                    def _invoke_rewrite():
                        response = self.llm.invoke(prompt)
                        return response.content.strip() if hasattr(response, 'content') else str(response).strip()
                    
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        future = executor.submit(_invoke_rewrite)
                        try:
                            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¶…æ™‚æ™‚é–“
                            rewritten_query = future.result(timeout=OLLAMA_QUERY_OPTIMIZATION_TIMEOUT)
                            
                            # å¢å¼·çš„è³ªé‡æª¢æŸ¥ï¼Œåƒç…§å‚³çµ±RAG
                            if not rewritten_query or len(rewritten_query.strip()) < 2:
                                if attempt < OLLAMA_MAX_RETRIES - 1:
                                    logger.warning(f"æŸ¥è©¢é‡å¯«çµæœéçŸ­ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦...")
                                    time.sleep(OLLAMA_RETRY_DELAY)
                                    continue
                                return original_query
                            
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«åŸæŸ¥è©¢çš„é—œéµæ¦‚å¿µ
                            orig_words = set(original_query.lower().split())
                            rewrite_words = set(rewritten_query.lower().split())
                            if len(orig_words & rewrite_words) == 0 and len(orig_words) > 1:
                                logger.warning(f"é‡å¯«æŸ¥è©¢èˆ‡åŸæŸ¥è©¢é—œè¯åº¦ä½ï¼Œå˜—è©¦ {attempt + 1} æ¬¡é‡è©¦...")
                                if attempt < OLLAMA_MAX_RETRIES - 1:
                                    time.sleep(OLLAMA_RETRY_DELAY) 
                                    continue
                                return original_query
                            
                            # æª¢æŸ¥æ¨™é»ç¬¦è™Ÿéå¤šçš„å•é¡Œ
                            punctuation_count = sum(1 for char in rewritten_query if char in 'ï¼Œ,ã€‚.ï¼!ï¼Ÿ?ï¼›;ï¼š:')
                            if punctuation_count > len(rewritten_query) * 0.3:
                                if attempt < OLLAMA_MAX_RETRIES - 1:
                                    logger.warning(f"é‡å¯«æŸ¥è©¢æ¨™é»éå¤šï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦...")
                                    time.sleep(OLLAMA_RETRY_DELAY)
                                    continue
                                return original_query
                            
                            # æª¢æŸ¥é•·åº¦æ˜¯å¦åˆç† (ä¸è¶…éåŸæŸ¥è©¢çš„3å€)
                            if len(rewritten_query) > len(original_query) * 3:
                                if attempt < OLLAMA_MAX_RETRIES - 1:
                                    logger.warning(f"é‡å¯«æŸ¥è©¢éé•·ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦...")
                                    time.sleep(OLLAMA_RETRY_DELAY)
                                    continue
                                return original_query

                            logger.info(f"ğŸ” å„ªåŒ–å¾ŒæŸ¥è©¢: {rewritten_query}")
                            return rewritten_query
                            
                        except concurrent.futures.TimeoutError:
                            if attempt < OLLAMA_MAX_RETRIES - 1:
                                logger.warning(f"æŸ¥è©¢é‡å¯«è¶…æ™‚ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦...")
                                time.sleep(OLLAMA_RETRY_DELAY)
                                continue
                            else:
                                logger.error("æŸ¥è©¢é‡å¯«å¤šæ¬¡è¶…æ™‚ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢")
                                return original_query
                
                except Exception as e:
                    if attempt < OLLAMA_MAX_RETRIES - 1:
                        logger.warning(f"æŸ¥è©¢é‡å¯«å‡ºéŒ¯ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è©¦: {str(e)}")
                        time.sleep(OLLAMA_RETRY_DELAY)
                        continue
                    else:
                        logger.error(f"æŸ¥è©¢é‡å¯«å¤šæ¬¡å¤±æ•—: {str(e)}")
                        return original_query
            
            return original_query
            
        except Exception as e:
            logger.error(f"æŸ¥è©¢é‡å¯«å¤±æ•—: {str(e)}")
            return original_query
    
    def get_file_count_warning(self) -> str:
        """ç²å–æ–‡ä»¶æ•¸é‡è­¦å‘Š"""
        return getattr(self.file_retriever, '_file_count_warning', None)

    def get_scope_info(self) -> Dict[str, Any]:
        """è¿”å›ç›®å‰æª¢ç´¢ç¯„åœçš„çµ±è¨ˆèˆ‡è­¦å‘Šè³‡è¨Š"""
        fr = self.file_retriever
        meta = getattr(fr, '_estimation_meta', {}) if hasattr(fr, '_estimation_meta') else {}
        from .dynamic_rag_base import SECURITY_EVENTS  # self import safe for runtime
        sec_count = len(SECURITY_EVENTS)
        last_sec = SECURITY_EVENTS[-1] if sec_count else None
        # å®‰å…¨äº‹ä»¶é€Ÿç‡ç›£æ§ (æœ€è¿‘10åˆ†é˜)
        recent_10m = 0
        rate_flag = None
        now_ts = int(time.time())
        try:
            for ev in reversed(SECURITY_EVENTS):
                if now_ts - ev['ts'] > 600:
                    break
                recent_10m += 1
            if recent_10m >= 15:
                rate_flag = 'critical'
            elif recent_10m >= 8:
                rate_flag = 'elevated'
        except Exception:
            pass
        return {
            "folder_limited": fr.folder_path != fr.base_path,
            "estimated_file_count": fr._estimated_file_count,
            "file_count_warning": fr._file_count_warning,
            "file_count_warning_level": fr._file_count_warning_level,
            "effective_folder": str(fr.folder_path),
            "base_path": str(fr.base_path),
            "estimation_confidence": meta.get('confidence'),
            "estimation_method": meta.get('method'),
            "estimation_sampled_dirs": meta.get('sampled_dirs'),
            "estimation_total_dirs": meta.get('total_dirs_seen'),
            "estimation_mean_per_dir": meta.get('mean_per_dir'),
            "estimation_ci_width": meta.get('ci_width'),
            "actual_cached_files": getattr(fr, '_actual_file_cache_size', None),
            "scan_truncated": getattr(fr, '_scan_truncated', None),
            "security_event_count": sec_count,
            "last_security_event": last_sec,
            "security_recent_10m": recent_10m,
            "security_rate_flag": rate_flag
        }
    
    def answer_question(self, question: str) -> str:
        """å›ç­”å•é¡Œ - å‹•æ…‹RAGæµç¨‹ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼Œå¢åŠ æª”æ¡ˆæ•¸é‡æ§åˆ¶ï¼‰"""
        try:
            logger.info(f"é–‹å§‹å‹•æ…‹RAGè™•ç†: {question}")
            
            # æª¢æŸ¥æ–‡ä»¶æ•¸é‡è­¦å‘Š
            warning = self.get_file_count_warning()
            if warning and self._file_count_warning_level in ["critical", "high"]:
                # å°æ–¼å¤§é‡æª”æ¡ˆï¼Œæå‰è¿”å›å»ºè­°
                return f"âš ï¸  {warning}\n\nç‚ºäº†ç²å¾—æ›´å¥½çš„æœç´¢æ•ˆæœï¼Œè«‹ï¼š\n1. åœ¨å‰ç«¯ä»‹é¢é¸æ“‡ã€Œé™åˆ¶æœç´¢ç¯„åœã€\n2. é¸æ“‡ç‰¹å®šçš„è³‡æ–™å¤¾é€²è¡Œæœç´¢\n3. ä½¿ç”¨æ›´å…·é«”çš„é—œéµè©\n\nè‹¥è¦å¼·åˆ¶æœç´¢å…¨éƒ¨ç¯„åœï¼Œè«‹é‡æ–°æå•ä¸¦ä½¿ç”¨æ›´ç²¾ç¢ºçš„é—œéµè©ã€‚"
            
            # è¨˜éŒ„æ–‡ä»¶å¤¾é™åˆ¶ä¿¡æ¯
            if self.folder_path:
                logger.info(f"ğŸ”’ æœç´¢ç¯„åœé™åˆ¶åœ¨æ–‡ä»¶å¤¾: {self.folder_path}")
            
            # 1. æŸ¥è©¢é‡å¯«å„ªåŒ–
            optimized_query = self.rewrite_query(question)
            
            # 2. æª¢ç´¢ç›¸é—œæ–‡ä»¶ï¼ˆå¢åŠ æ•¸é‡ï¼‰
            relevant_files = self.file_retriever.retrieve_relevant_files(optimized_query, max_files=15)
            
            # è¨˜éŒ„æ–‡ä»¶å¤¾é™åˆ¶çš„çµæœ
            if self.folder_path:
                logger.info(f"ğŸ”’ æ–‡ä»¶å¤¾é™åˆ¶ '{self.folder_path}' å·²åœ¨æª¢ç´¢éšæ®µç”Ÿæ•ˆï¼Œæ‰¾åˆ° {len(relevant_files)} å€‹ç›¸é—œæ–‡ä»¶")
            
            if not relevant_files:
                # æª¢æŸ¥æ˜¯å¦å› ç‚ºæ²’æœ‰é™åˆ¶ç¯„åœè€Œç„¡çµæœ
                if not self.folder_path and hasattr(self.file_retriever, '_file_count_warning_level'):
                    if self.file_retriever._file_count_warning_level in ["critical", "high"]:
                        return f"åœ¨å¤§é‡æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æ˜ç¢ºç›¸é—œçš„æ–‡æª”ã€‚\n\nå»ºè­°ï¼š\n1. é¸æ“‡ç‰¹å®šè³‡æ–™å¤¾é™åˆ¶æœç´¢ç¯„åœ\n2. ä½¿ç”¨æ›´å…·é«”çš„é—œéµè©\n3. æª¢æŸ¥é—œéµè©æ˜¯å¦æ­£ç¢º"
                
                return self._generate_general_knowledge_answer(question)
            
            # 3. è™•ç†æ–‡ä»¶å…§å®¹
            documents = self.content_processor.process_files(relevant_files)
            
            if not documents:
                return self._generate_general_knowledge_answer(question)
            
            # 4. å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è¨ˆç®—
            query_vector = self.vectorizer.vectorize_query(optimized_query)
            doc_vectors = self.vectorizer.vectorize_documents(documents)
            similarities = self.vectorizer.calculate_similarities(query_vector, doc_vectors)
            
            # 5. ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–‡æª”é¸æ“‡ç­–ç•¥
            top_docs = self._select_best_documents(similarities, question)
            # 5.1 é€²è¡Œèªç¾©å†—é¤˜è£å‰ªï¼šç§»é™¤é«˜åº¦ç›¸ä¼¼çš„æ–‡æª”ä»¥å£“ç¸®ä¸Šä¸‹æ–‡ï¼ˆé¿å…å¤šä»½è¿‘ä¼¼æ®µè½ï¼‰
            try:
                if top_docs and len(top_docs) > 2:
                    pruned = self._prune_redundant_documents(top_docs)
                    if pruned and len(pruned) < len(top_docs):
                        logger.info(f"Semantic redundancy pruning: {len(top_docs)} -> {len(pruned)} docs")
                        top_docs = pruned
            except Exception as _prune_e:
                logger.debug(f"Pruning skipped due to error: {_prune_e}")
            
            if not top_docs:
                return self._generate_general_knowledge_answer(question)
            
            # 6. ç”Ÿæˆè±å¯Œçš„ä¸Šä¸‹æ–‡
            context = self._format_enhanced_context(top_docs)
            answer = self._generate_answer(question, context)
            
            return answer
            
        except Exception as e:
            logger.error(f"å‹•æ…‹RAGè™•ç†å¤±æ•—: {str(e)}")
            return self._get_error_message()

    def _prune_redundant_documents(self, documents: List[Document], similarity_threshold: Optional[float] = None) -> List[Document]:
        """åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦å»é™¤èªç¾©é«˜åº¦é‡è¤‡çš„æ–‡æª”ï¼ˆè‡ªé©æ‡‰ï¼‰ã€‚

        è‡ªé©æ‡‰ç­–ç•¥ï¼š
        1. åµŒå…¥æ‰€æœ‰å€™é¸æ–‡æœ¬å‰ 512 å­—ã€‚
        2. è‹¥æœªæŒ‡å®š similarity_thresholdï¼Œè¨ˆç®—å…¨éƒ¨å‘é‡å…©å…©é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†ä½ˆï¼ˆä¸Šä¸‰è§’ï¼‰ã€‚
           - å–å¾—å‡å€¼ Î¼ èˆ‡æ¨™æº–å·® Ïƒ
           - å»ºè­°é–¾å€¼ = clamp( Î¼ + 0.35 * Ïƒ , 0.82, 0.92 )
           - ç”¨æ–¼å…¼é¡§é›†ä¸­/åˆ†æ•£èªæ–™ï¼šèšé›†æ€§å¼· -> é–¾å€¼ç¨é«˜æ¸›å°‘åˆªé™¤éå¤šï¼›åˆ†æ•£ -> é–¾å€¼é™ä½æå‡è£å‰ªã€‚
        3. è¿­ä»£ä¿åºéæ¿¾ï¼šèˆ‡å·²ä¿ç•™é›†åˆæœ€é«˜ç›¸ä¼¼åº¦ >= é–¾å€¼ -> è¦–ç‚ºå†—é¤˜è·³éã€‚
        4. ä¿ç•™è‡³å°‘ 2 å€‹æ–‡æª”ï¼Œé¿å…éåº¦è£å‰ªã€‚
        Args:
            documents: åˆæ­¥é¸æ“‡çš„æ–‡æª”åˆ—è¡¨ã€‚
            similarity_threshold: è‹¥æä¾›ï¼Œä½¿ç”¨å›ºå®šå€¼ï¼›å¦å‰‡è‡ªé©æ‡‰ã€‚
        """
        if len(documents) <= 2:  # ä¸è£å‰ªæ¥µå°‘é‡
            return documents
        try:
            import numpy as _np
            texts = [d.page_content[:512] for d in documents]
            vectors = self.vectorizer.embeddings.embed_documents(texts)
            vectors_np = [_np.array(v) for v in vectors]
            # è‡ªé©æ‡‰é–¾å€¼è¨ˆç®—
            if similarity_threshold is None:
                sims_all = []
                for i in range(len(vectors_np)):
                    vi = vectors_np[i]
                    n1 = _np.linalg.norm(vi) or 1.0
                    for j in range(i+1, len(vectors_np)):
                        vj = vectors_np[j]
                        denom = (n1 * (_np.linalg.norm(vj) or 1.0)) or 1.0
                        sims_all.append(float(_np.dot(vi, vj) / denom))
                if sims_all:
                    mu = float(_np.mean(sims_all))
                    sigma = float(_np.std(sims_all))
                    adaptive = mu + 0.35 * sigma
                    similarity_threshold = min(0.92, max(0.82, adaptive))
                else:
                    similarity_threshold = 0.9
                try:
                    logger.debug(f"[AdaptivePrune] sims_count={len(sims_all)} mu={mu:.3f} Ïƒ={sigma:.3f} threshold={similarity_threshold:.3f}")
                except Exception:
                    pass
            kept_indices: List[int] = []
            kept_vecs: List[_np.ndarray] = []
            for idx, v in enumerate(vectors_np):
                if not kept_vecs:
                    kept_indices.append(idx)
                    kept_vecs.append(v)
                    continue
                # è¨ˆç®—èˆ‡å·²ä¿ç•™æœ€å¤§ç›¸ä¼¼åº¦
                n_v = _np.linalg.norm(v) or 1.0
                max_sim = -1.0
                for kv in kept_vecs:
                    denom = (n_v * (_np.linalg.norm(kv) or 1.0)) or 1.0
                    sim = float(_np.dot(v, kv) / denom)
                    if sim > max_sim:
                        max_sim = sim
                        if max_sim >= (similarity_threshold or 0.9):
                            break
                if max_sim < (similarity_threshold or 0.9):
                    kept_indices.append(idx)
                    kept_vecs.append(v)
            # ä¿è­‰è‡³å°‘ 2 å€‹
            if len(kept_indices) < 2 and len(documents) >= 2:
                kept_indices = [0, 1]
            return [documents[i] for i in kept_indices]
        except Exception as _e:
            logger.debug(f"Adaptive pruning error: {_e}")
            return documents
    
    def _select_best_documents(self, similarities: List[Tuple[Document, float]], question: str) -> List[Document]:
        """
        æ™ºèƒ½é¸æ“‡æœ€ä½³æ–‡æª” - åƒç…§å‚³çµ±RAGçš„æ–‡æª”é¸æ“‡ç­–ç•¥
        
        Args:
            similarities: æ–‡æª”ç›¸ä¼¼åº¦åˆ—è¡¨
            question: åŸå§‹å•é¡Œ
            
        Returns:
            é¸ä¸­çš„æ–‡æª”åˆ—è¡¨
        """
        if not similarities:
            return []

        from config.config import SIMILARITY_TOP_K
        max_docs = min(SIMILARITY_TOP_K, 10)

        # ç¬¬ä¸€è¼ªï¼šæ”¶é›†æ¯å€‹æ–‡ä»¶çš„æœ€ä½³åˆ†æ•¸
        file_best_scores = {}
        for doc, score in similarities:
            file_path = doc.metadata.get('file_path', 'unknown')
            if file_path not in file_best_scores or score > file_best_scores[file_path]:
                file_best_scores[file_path] = score

        # è¨ˆç®—å‹•æ…‹é–¾å€¼ (ç›¸ä¼¼åº¦è¶Šé«˜è¶Šå¥½)
        if file_best_scores:
            scores = list(file_best_scores.values())
            avg_score = sum(scores) / len(scores)
            # è¨­ç½®ä¸€å€‹åˆç†çš„é–¾å€¼ï¼Œä¾‹å¦‚å¹³å‡åˆ†çš„80%ï¼Œä½†ä¸ä½æ–¼ä¸€å€‹çµ•å°å€¼
            dynamic_threshold = max(avg_score * 0.8, 0.4)
            logger.info(f"å‹•æ…‹é–¾å€¼è¨ˆç®—: å¹³å‡åˆ†={avg_score:.3f}, æœ€çµ‚é–¾å€¼={dynamic_threshold:.3f}")
        else:
            dynamic_threshold = 0.5

        # ç¬¬äºŒè¼ªï¼šæŒ‰æ–‡ä»¶å»é‡ï¼Œä¸¦æ ¹æ“šé–¾å€¼ç¯©é¸
        selected_docs = []
        seen_files = set()
        # æŒ‰åˆ†æ•¸å¾é«˜åˆ°ä½æ’åº
        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

        for doc, score in sorted_similarities:
            if len(selected_docs) >= max_docs:
                break
            
            file_path = doc.metadata.get('file_path', 'unknown')
            if file_path not in seen_files:
                if score >= dynamic_threshold:
                    doc.metadata['score'] = score
                    selected_docs.append(doc)
                    seen_files.add(file_path)

        # å¦‚æœç¯©é¸å¾Œæ–‡æª”éå°‘ï¼Œå‰‡æ”¾å¯¬æ¢ä»¶ï¼Œå–åˆ†æ•¸æœ€é«˜çš„å¹¾å€‹
        if not selected_docs and sorted_similarities:
            logger.info("æ²’æœ‰æ–‡æª”é€šéå‹•æ…‹é–¾å€¼ï¼Œæ”¾å¯¬æ¢ä»¶é¸å–æœ€é«˜åˆ†çš„3å€‹æ–‡æª”")
            # ç¢ºä¿æ¯å€‹æ–‡ä»¶åªé¸ä¸€æ¬¡
            top_files = {}
            for doc, score in sorted_similarities:
                file_path = doc.metadata.get('file_path', 'unknown')
                if file_path not in top_files:
                    top_files[file_path] = doc
                    if len(top_files) >= 3:
                        break
            selected_docs = list(top_files.values())

        logger.info(f"é¸æ“‡äº† {len(selected_docs)} å€‹æ–‡æª”")
        return selected_docs
    
    def _format_enhanced_context(self, documents: List[Document]) -> str:
        """
        æ ¼å¼åŒ–å¢å¼·ä¸Šä¸‹æ–‡ - åƒè€ƒå‚³çµ±RAGçš„åšæ³•ï¼Œä½¿ç”¨æ›´çµæ§‹åŒ–çš„æ ¼å¼
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        """
        context_parts = []
        max_total_length = 4000  # é™åˆ¶ç¸½é•·åº¦
        current_length = 0
        
        for i, doc in enumerate(documents, 1):
            file_name = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            relative_path = doc.metadata.get("file_path", "æœªçŸ¥è·¯å¾‘")
            content = doc.page_content.strip()
            
            if content and current_length < max_total_length:
                # è¨ˆç®—å¯ç”¨é•·åº¦
                header = f"--- ç›¸é—œæ–‡ä»¶ {i} ---\nä¾†æº: {file_name}\nè·¯å¾‘: {relative_path}\n"
                available_length = max_total_length - current_length - len(header)
                
                if available_length <= 0:
                    break

                if len(content) > available_length:
                    content = content[:available_length] + "... (å…§å®¹æˆªæ–·)"
                
                context_parts.append(f"{header}å…§å®¹æ‘˜è¦:\n{content}\n")
                current_length += len(header) + len(content)
        
        return "\n".join(context_parts)

    
    def _format_context(self, documents: List[Document]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, doc in enumerate(documents, 1):
            file_name = doc.metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
            content = doc.page_content.strip()
            context_parts.append(f"ç›¸é—œå…§å®¹ {i} (ä¾†æº: {file_name}):\n{content}\n")
        
        return "\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            if self.llm is None:
                return f"æ ¹æ“šæ–‡æª”å…§å®¹ï¼Œé—œæ–¼ã€Œ{question}ã€çš„ä¿¡æ¯å¦‚ä¸‹ï¼š\n\n{context[:500]}...\n\næ³¨æ„ï¼šDynamic RAG èªè¨€æ¨¡å‹æš«æ™‚ç¦ç”¨ï¼Œé€™æ˜¯åŸºæ–¼æª¢ç´¢å…§å®¹çš„ç°¡åŒ–å›ç­”ã€‚"
            
            prompt = self.ANSWER_PROMPT_TEMPLATE.format(context=context, question=question)
            # èªè¨€ç²¾ç°¡æŒ‡ä»¤æ”¹ç‚ºç”±å­é¡æä¾›ï¼ˆé¿å…å…±ç”¨è€¦åˆï¼‰
            try:
                prefix = self.get_concise_prefix()
                if prefix:
                    prompt = prefix + prompt
            except Exception:
                pass
            
            response = self.llm.invoke(prompt)
            result = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            # æª¢æŸ¥å›ç­”é•·åº¦ï¼Œå¦‚æœå¤ªçŸ­å‰‡ä½¿ç”¨å›é€€æ–¹æ³•
            if not result or len(result.strip()) < 5:
                return self._get_general_fallback(question)
            
            # æ¸…ç†èˆ‡å£“ç¸®å›ç­”ï¼Œç§»é™¤æ¨¡å‹ç”¢ç”Ÿçš„é›œè¨Š/é‡è¤‡/æ ¡æ­£æ®µè½
            cleaned = self._clean_answer_text(result)
            return self._ensure_language(cleaned)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # æä¾›åŒ…å«ä¸Šä¸‹æ–‡çš„å›é€€ç­”æ¡ˆ
            return self._generate_fallback_answer(question, context)

    def _generate_fallback_answer(self, question: str, context: str) -> str:
        """ç•¶LLMèª¿ç”¨å¤±æ•—æ™‚ï¼Œæä¾›ä¸€å€‹åŸºæ–¼ä¸Šä¸‹æ–‡çš„ç°¡åŒ–å›ç­”"""
        return f"æŠ±æ­‰ï¼ŒAIæ¨¡å‹åœ¨ç”Ÿæˆå›ç­”æ™‚é‡åˆ°å•é¡Œã€‚åŸºæ–¼æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼Œä»¥ä¸‹æ˜¯ç›¸é—œæ‘˜è¦ï¼š\n\n{context[:1000]}...\n\nè«‹æ‚¨æ ¹æ“šé€™äº›ä¿¡æ¯è‡ªè¡Œåˆ¤æ–·ã€‚"

    
    def _generate_general_knowledge_answer(self, question: str) -> str:
        """ç”Ÿæˆå¸¸è­˜å›ç­” - ç•¶æ‰¾ä¸åˆ°ç›¸é—œæ–‡æª”æ™‚ï¼ŒåŸºæ–¼å¸¸è­˜æä¾›å›ç­”ï¼ˆå­é¡å¯¦ç¾ï¼‰"""
        # é»˜èªå¯¦ç¾ï¼Œå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•
        return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æª”ä¸­æ‰¾ä¸åˆ°èˆ‡ã€Œ{question}ã€ç›¸é—œçš„å…·é«”ä¿¡æ¯ã€‚é€™å¯èƒ½æ˜¯å› ç‚ºç›¸é—œæ–‡æª”ä¸åœ¨ç•¶å‰çš„æª¢ç´¢ç¯„åœå…§ï¼Œæˆ–è€…å•é¡Œæ¶‰åŠçš„å…§å®¹éœ€è¦æ›´å…·é«”çš„é—œéµè©ã€‚å»ºè­°æ‚¨å˜—è©¦ä½¿ç”¨æ›´å…·é«”çš„é—œéµè©é‡æ–°æå•ã€‚"
    
    def _ensure_language(self, result: str) -> str:
        """ç¢ºä¿è¼¸å‡ºç¬¦åˆç›®æ¨™èªè¨€ï¼ˆå­é¡å¯é‡å¯«ï¼‰"""
        return result
    
    def _get_error_message(self) -> str:
        """ç²å–éŒ¯èª¤æ¶ˆæ¯ï¼ˆå­é¡å¯é‡å¯«ï¼‰"""
        return "è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    def _get_general_fallback(self, query: str) -> str:
        """ç²å–é€šç”¨å›é€€å›ç­”ï¼ˆå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•ï¼‰"""
        # é»˜èªå¯¦ç¾ï¼Œå­é¡æ‡‰è©²é‡å¯«æ­¤æ–¹æ³•
        return f"æ ¹æ“šä¸€èˆ¬ITçŸ¥è­˜ï¼Œé—œæ–¼ã€Œ{query}ã€çš„ç›¸é—œä¿¡æ¯å¯èƒ½éœ€è¦æŸ¥é–±æ›´å¤šQSIå…§éƒ¨æ–‡æª”ã€‚"

    # ---- æ–°å¢ï¼šç”±å­é¡è¦†å¯«çš„ç²¾ç°¡å›ç­”å‰ç¶´é‰¤å­ ----
    def get_concise_prefix(self) -> str:
        """è¿”å›èªè¨€/å ´æ™¯å°ˆå±¬çš„ç²¾ç°¡æŒ‡ä»¤å‰ç¶´ã€‚

        å­é¡å¯è¦†å¯«ä»¥æä¾›ï¼šé™åˆ¶è¡Œæ•¸ã€é¿å…é‡è¤‡/è‡ªæˆ‘ä¿®æ­£/è´…èª ç­‰ç­–ç•¥ã€‚
        é»˜èªè¿”å›ç©ºå­—ä¸²ä»£è¡¨ä¸æ·»åŠ é¡å¤–å‰ç¶´ã€‚
        """
        return ""
    
    def generate_relevance_reason(self, question: str, doc_content: str) -> str:
        """ç”Ÿæˆç›¸é—œæ€§ç†ç”±"""
        if not question or not question.strip() or not doc_content or not doc_content.strip():
            return "ç„¡æ³•ç”Ÿæˆç›¸é—œæ€§ç†ç”±ï¼šæŸ¥è©¢æˆ–æ–‡æª”ç‚ºç©º"

        try:
            trimmed_content = doc_content[:1000].strip()
            prompt = self.RELEVANCE_PROMPT_TEMPLATE.format(question=question, trimmed_content=trimmed_content)
            response = self.llm.invoke(prompt)
            reason = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            return reason if reason else "ç„¡æ³•ç¢ºå®šç›¸é—œæ€§ç†ç”±"
        except Exception as e:
            logger.error(f"ç”Ÿæˆç›¸é—œæ€§ç†ç”±æ™‚å‡ºéŒ¯: {str(e)}")
            return "ç”Ÿæˆç›¸é—œæ€§ç†ç”±å¤±æ•—"

    def generate_batch_relevance_reasons(self, question: str, documents: List[Document]) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆå¤šå€‹æ–‡æª”çš„ç›¸é—œæ€§ç†ç”±ï¼Œæé«˜æ•ˆèƒ½"""
        doc_contents = [doc.page_content for doc in documents]
        if not question or not question.strip() or not doc_contents:
            return ["ç„¡æ³•ç”Ÿæˆç›¸é—œæ€§ç†ç”±"] * len(doc_contents)

        # Check if the batch prompt is implemented in the subclass
        if not self.BATCH_RELEVANCE_PROMPT_TEMPLATE or not self.BATCH_RELEVANCE_PROMPT_TEMPLATE.strip():
            logger.warning("æ‰¹é‡ç›¸é—œæ€§ç†ç”±æ¨¡æ¿æœªå¯¦ç¾ï¼Œé€€å›è‡³å–®å€‹ç”Ÿæˆæ¨¡å¼ã€‚")
            return [self.generate_relevance_reason(question, content) for content in doc_contents]

        try:
            docs_text = ""
            for i, content in enumerate(doc_contents, 1):
                docs_text += f"æ–‡æª”{i}: {content[:500]}...\n\n" # Limit content length

            prompt = self.BATCH_RELEVANCE_PROMPT_TEMPLATE.format(question=question, docs_text=docs_text)
            
            response = self.llm.invoke(prompt)
            batch_result = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            reasons = []
            lines = batch_result.strip().split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith(('1.', '2.', '3.', '4.', '5.')) or '. ' in line[:4]):
                    reason = line.split('.', 1)[1].strip()
                    reasons.append(reason)
            
            if len(reasons) != len(doc_contents):
                logger.warning(f"æ‰¹é‡ç›¸é—œæ€§ç†ç”±è§£æå¤±æ•—ã€‚é æœŸ {len(doc_contents)} å€‹ï¼Œå¾—åˆ° {len(reasons)} å€‹ã€‚é€€å›è‡³å–®å€‹ç”Ÿæˆã€‚")
                return [self.generate_relevance_reason(question, content) for content in doc_contents]

            return reasons

        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆç›¸é—œæ€§ç†ç”±æ™‚å‡ºéŒ¯: {str(e)}ã€‚é€€å›è‡³å–®å€‹ç”Ÿæˆã€‚")
            return [self.generate_relevance_reason(question, content) for content in doc_contents]
    
    def _get_no_docs_message(self) -> str:
        return "æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”"
    
    def _get_error_message(self) -> str:
        return "ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\n\nğŸ’¡ é€™å¯èƒ½æ˜¯å› ç‚ºæ¨¡å‹å°šæœªå®Œå…¨ä¸‹è¼‰æˆ–åˆå§‹åŒ–ã€‚å¦‚æœæ‚¨æ˜¯é¦–æ¬¡ä½¿ç”¨ï¼Œè«‹ç­‰å¾…æ¨¡å‹ä¸‹è¼‰å®Œæˆå¾Œå†è©¦ã€‚\n\nå»ºè­°ï¼š\n- æª¢æŸ¥ç¶²è·¯é€£æ¥\n- é¸æ“‡è¼ƒå°çš„æ¨¡å‹é€²è¡Œæ¸¬è©¦\n- æŸ¥çœ‹ç³»çµ±ç‹€æ…‹ç¢ºèªæ¨¡å‹æ˜¯å¦å°±ç·’"
    
    def _get_timeout_message(self) -> str:
        return "è™•ç†è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦"
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Document]:
        """å‹•æ…‹æª¢ç´¢æ–‡æª”"""
        try:
            relevant_files = self.file_retriever.retrieve_relevant_files(query, max_files=top_k * 2)
            documents = self.content_processor.process_files(relevant_files)
            
            if not documents:
                return []
            
            query_vector = self.vectorizer.vectorize_query(query)
            doc_vectors = self.vectorizer.vectorize_documents(documents)
            similarities = self.vectorizer.calculate_similarities(query_vector, doc_vectors)
            
            return [doc for doc, score in similarities[:top_k] if score > 0.2]
            
        except Exception as e:
            logger.error(f"å‹•æ…‹æª¢ç´¢æ–‡æª”å¤±æ•—: {str(e)}")
            return []

    def _ensure_language(self, text: str) -> str:
        """åœ¨å¿…è¦æ™‚å°‡è¼¸å‡ºè½‰æ›ç‚ºç›®æ¨™èªè¨€ï¼Œç¢ºä¿æœ€çµ‚å›ç­”èªè¨€ä¸€è‡´"""
        try:
            target_lang = None
            try:
                target_lang = self.get_language()
            except Exception:
                target_lang = None

            if not target_lang or not text or len(text) < 5:
                return text

            # ç°¡å–®èªè¨€ç‰¹å¾µçµ±è¨ˆ
            total_len = max(1, len(text))
            ascii_letters = sum(1 for c in text if c.isascii() and c.isalpha())
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            thai_chars = sum(1 for c in text if '\u0e00' <= c <= '\u0e7f')

            ascii_ratio = ascii_letters / total_len
            zh_ratio = chinese_chars / total_len
            th_ratio = thai_chars / total_len

            def translate(to_lang_prompt: str) -> str:
                try:
                    translate_prompt = f"{to_lang_prompt}\n\nâ€”â€”â€”\n{text}\nâ€”â€”â€”\nåªè¼¸å‡ºç¿»è­¯çµæœã€‚"
                    resp = self.llm.invoke(translate_prompt)
                    return resp.content.strip() if hasattr(resp, 'content') else str(resp).strip()
                except Exception:
                    return text

            if target_lang in ("ç¹é«”ä¸­æ–‡", "ç®€ä½“ä¸­æ–‡"):
                # ä¸»è¦ç‚ºä¸­æ–‡ï¼Œè‹¥ä¸­æ–‡æ¯”ä¾‹éä½ä¸”è‹±æ–‡æ¯”ä¾‹é«˜ï¼Œå˜—è©¦ç¿»è­¯
                if zh_ratio < 0.20 and ascii_ratio > 0.50:
                    return translate("è«‹å°‡ä»¥ä¸‹å…§å®¹ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡") if target_lang == "ç¹é«”ä¸­æ–‡" else translate("è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡")
                return text

            if target_lang == "English":
                # ä¸»è¦ç‚ºè‹±æ–‡ï¼Œè‹¥è‹±æ–‡æ¯”ä¾‹éä½ä½†ä¸­æ–‡æˆ–æ³°æ–‡æ¯”ä¾‹è¼ƒé«˜ï¼Œå˜—è©¦ç¿»è­¯
                if ascii_ratio < 0.40 and (zh_ratio > 0.20 or th_ratio > 0.20):
                    return translate("Please translate the following content into English")
                return text

            if target_lang == "à¹„à¸—à¸¢":
                if th_ratio < 0.10 and (ascii_ratio > 0.50 or zh_ratio > 0.20):
                    return translate("à¹‚à¸›à¸£à¸”à¹à¸›à¸¥à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢")
                return text

            return text
        except Exception:
            return text

    def _clean_answer_text(self, text: str) -> str:
        """é€šç”¨å›ç­”æ¸…ç†ï¼šç§»é™¤ç•°å¸¸ç¬¦è™Ÿã€é‡è¤‡ä¿®æ­£æ®µã€ç ´ç¢å­—å…ƒèˆ‡å†—é¤˜å°¾è¨»ï¼ˆæ¨¡å‹ç„¡é—œï¼‰ã€‚"""
        import re
        original_len = len(text)
        # ç§»é™¤å¥‡æ€ªçš„åˆ†éš”/é›¶å¯¬/æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)
        text = ''.join(ch for ch in text if (ch.isprintable() or ch in '\n\r\t'))
        # åˆä½µè¢«ç©ºæ ¼æ‹†é–‹çš„è‹±æ–‡å­—æ¯ï¼ˆK P I -> KPIï¼‰
        text = re.sub(r'\b([A-Za-z])\s+([A-Za-z])\s+([A-Za-z])\b', lambda m: ''.join(m.groups()), text)
        text = re.sub(r'\b([A-Za-z])\s+([A-Za-z])\b', lambda m: ''.join(m.groups()), text)
        # ç§»é™¤å¤šæ¬¡ã€Œå†æ¬¡ä¿®æ­£ã€ã€Œæœ€çµ‚ç‰ˆæœ¬ã€ç­‰é‡è¤‡èªªæ˜æ®µè½ï¼Œåªä¿ç•™ç¬¬ä¸€æ¬¡
        lines = [l.strip() for l in text.splitlines() if l.strip()]
        filtered = []
        seen_tags = set()
        noise_markers = ['å†æ¬¡ä¿®æ­£', 'æœ€çµ‚ç‰ˆæœ¬', 'å†ä¸€æ¬¡ä¿®æ­£', 'éŒ¯èª¤ç‰ˆæœ¬', 'å·²ä¿®æ­£', 'ï¼ˆæœ€çµ‚ç‰ˆæœ¬ï¼‰']
        disclaimer_markers = [
            'ä»¥ä¸Šå›ç­”', 'åƒ…ä¾›åƒè€ƒ', 'Note:', 'The above answer', 'Disclaimer', 'å…è²¬', 'æ³¨æ„ï¼šä»¥ä¸Šå›ç­”']
        for ln in lines:
            if any(tag in ln for tag in noise_markers):
                key = ''.join(ch for ch in ln if ch.isalnum())[:30]
                if key in seen_tags:
                    continue
                seen_tags.add(key)
            # åˆä½µå¤šæ¬¡å…è²¬/èªªæ˜æ€§å°¾æ®µï¼Œåªä¿ç•™ç¬¬ä¸€å€‹
            if any(dm.lower() in ln.lower() for dm in disclaimer_markers):
                key = 'disc'
                if key in seen_tags:
                    continue
                seen_tags.add(key)
            filtered.append(ln)
        text = '\n'.join(filtered)
        # å»é™¤é€£çºŒé‡è¤‡æ®µè½
        paragraphs = [p.strip() for p in re.split(r'\n{2,}', text) if p.strip()]
        dedup = []
        prev_hash = None
        for p in paragraphs:
            h = hash(p[:120])
            if h == prev_hash:
                continue
            prev_hash = h
            dedup.append(p)
        text = '\n\n'.join(dedup)
        # è‹¥æ¸…ç†å¾Œéé•·ï¼Œæˆªæ–·æ–¼ 1200 å­—ç¬¦ä¸¦æ¨™è¨»
        if len(text) > 1200:
            text = text[:1200].rstrip() + '...'
        logger.debug(f"Answer cleaned from {original_len} -> {len(text)} chars")
        return text
    
    
