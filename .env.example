# Q槽路徑設置
Q_DRIVE_PATH="Q:/"
DISPLAY_DRIVE_NAME="Q:/"

# 向量數據庫設置
VECTOR_DB_PATH="./vector_db"  # 向量數據庫保存路徑
LOGS_DIR="./logs"              # 日誌目錄
BACKUPS_DIR="./backups"        # 備份目錄

# 文件類型設置
SUPPORTED_FILE_TYPES=".pdf,.docx,.doc,.xlsx,.xls,.txt,.md,.pptx,.ppt,.csv,.vsdx,.vsd"

# 應用設置
APP_HOST=127.0.0.1  # API主機地址
APP_PORT=8000         # API端口
STREAMLIT_PORT=8501   # Streamlit前端端口

# 其他設置
LOG_LEVEL="INFO"           # 日誌級別
MAX_TOKENS_CHUNK=500       # 文本塊大小
CHUNK_OVERLAP=100          # 塊重疊大小
SIMILARITY_TOP_K=10        # 相似度檢索返回的文檔數量

# 索引效能優化設置（針對大量文件優化）
MAX_WORKERS=1              # 同時處理線程數（降低避免競爭）
EMBEDDING_BATCH_SIZE=4     # 嵌入批處理大小（保守設置）
FILE_BATCH_SIZE=3          # 文件批處理大小（最小設置）
MAX_FILE_SIZE_MB=200        # 最大文件大小限制（MB）

# CUDA 記憶體管理設置 - 修復記憶體碎片化問題
PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,expandable_segments:False,garbage_collection_threshold:0.6"
CUDA_LAUNCH_BLOCKING=0
TORCH_CUDA_EMPTY_CACHE_FREQUENCY=10     # 每10次操作清理一次緩存
CUDA_VISIBLE_DEVICES="0,1"              # 可見的GPU設備

# 模型加載安全設置
MODEL_LOAD_MAX_RETRIES=2                # 模型加載最大重試次數
MODEL_LOAD_RETRY_DELAY=5                # 模型加載重試延遲（秒）
FORCE_MODEL_RELOAD=false                # 是否強制重新加載模型

# 智能回退模型配置（可根據需要自定義）
FALLBACK_MEDIUM_MODEL="microsoft/DialoGPT-medium"  # 中型回退模型
FALLBACK_SMALL_MODEL="microsoft/DialoGPT-small"    # 小型回退模型
FALLBACK_TINY_MODEL="distilgpt2"                   # 微型回退模型

# 編碼處理優化
AUTO_ENCODING_DETECTION=true    # 自動編碼檢測
USE_CHARDET=true               # 使用chardet庫
CLEAN_GARBLED_TEXT=true        # 清理亂碼文本

# 動態RAG優化設置（開發環境）
DYNAMIC_MAX_SCAN_FILES=1000    # 最大掃描文件數（保守設置）
DYNAMIC_SCAN_DEPTH=5           # 掃描深度（降低避免過深掃描）
DYNAMIC_CACHE_DURATION=600     # 緩存持續時間（秒，增加減少重複掃描）
DYNAMIC_FOLDER_SELECTION=true  # 啟用文件夾選擇功能

# 文件夾瀏覽優化設置（開發環境）
FOLDER_SCAN_MAX_FILES=50       # 文件夾掃描最大文件數（避免大文件夾超時）
FOLDER_SCAN_MAX_DEPTH=1        # 文件夾掃描最大深度（淺層掃描）
FOLDER_SCAN_TIMEOUT=5          # 文件夾掃描超時（秒）

# === Ollama 設置 ===
OLLAMA_HOST="http://localhost:11434"

# Ollama 超時設定（秒）- 優化後（開發環境）
OLLAMA_REQUEST_TIMEOUT=300              # 一般請求超時（增加到5分鐘）
OLLAMA_EMBEDDING_TIMEOUT=180            # 嵌入向量生成超時（增加到3分鐘）
OLLAMA_QUERY_OPTIMIZATION_TIMEOUT=90    # 查詢優化超時
OLLAMA_ANSWER_GENERATION_TIMEOUT=300    # 回答生成超時（增加到5分鐘）
OLLAMA_RELEVANCE_TIMEOUT=120            # 相關性分析超時（增加到2分鐘）
OLLAMA_CONNECTION_TIMEOUT=60            # 連接超時（增加到1分鐘）

# 重試機制設定
OLLAMA_MAX_RETRIES=3                    # 最大重試次數
OLLAMA_RETRY_DELAY=5                    # 重試延遲（秒）

# === Hugging Face 設置 ===
HF_MODEL_CACHE_DIR="./models/cache"     # Hugging Face 模型緩存目錄
HF_USE_GPU="true"                       # 是否使用 GPU
HF_TOKEN=""                             # Hugging Face API Token (可選)

# 模型設置 - 根據 GPU 記憶體自動選擇
DEFAULT_LLM_MODEL="Qwen/Qwen2-0.5B-Instruct"
DEFAULT_EMBEDDING_MODEL="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# 推理引擎配置（通過前端設置流程選擇，以下僅為默認值）
VLLM_GPU_MEMORY_UTILIZATION="0.7"      # GPU 記憶體使用率（降低避免記憶體問題）
VLLM_MAX_MODEL_LEN="4096"               # 最大序列長度
VLLM_TENSOR_PARALLEL_SIZE="2"           # 張量並行大小（雙GPU配置）

# PyTorch 設置 - 優化記憶體管理
TORCH_DEVICE="auto"                     # 設備：auto, cpu, cuda
TORCH_DTYPE="bfloat16"                  # 數據類型：bfloat16（更穩定）, float16, float32

# 環境設置
ENVIRONMENT="development"               # 環境：development, production

# 自訂的管理員安全密碼
ADMIN_TOKEN="your_admin_token_here"
