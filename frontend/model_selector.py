"""
ç°¡åŒ–çš„æ¨¡å‹é¸æ“‡çµ„ä»¶
"""

import streamlit as st
import requests
import json
from pathlib import Path
from typing import Dict, Any, Optional

def load_user_config():
    """è¼‰å…¥ç”¨æˆ¶é…ç½®"""
    config_file = Path("config/user_setup.json")
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}

def save_user_config(config: Dict[str, Any]):
    """ä¿å­˜ç”¨æˆ¶é…ç½®"""
    config_file = Path("config/user_setup.json")
    config_file.parent.mkdir(exist_ok=True)
    
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        st.error(f"ä¿å­˜é…ç½®å¤±æ•—: {str(e)}")
        return False

def render_model_selector(api_url: str):
    """æ¸²æŸ“ç°¡åŒ–çš„æ¨¡å‹é¸æ“‡å™¨"""
    
    st.sidebar.markdown("## âš™ï¸ æ¨¡å‹é…ç½®")
    
    # è¼‰å…¥ç•¶å‰é…ç½®
    current_config = load_user_config()
    
    # å¹³å°é¸æ“‡
    st.sidebar.markdown("### ğŸ¯ AI å¹³å°")
    platform_options = {
        "huggingface": "ğŸ¤— Hugging Face",
        "ollama": "ğŸ  Ollama"
    }
    
    selected_platform = st.sidebar.selectbox(
        "é¸æ“‡å¹³å°:",
        options=list(platform_options.keys()),
        format_func=lambda x: platform_options[x],
        index=0 if current_config.get("platform") == "huggingface" else 1,
        key="platform_selector"
    )
    
    # æ ¹æ“šå¹³å°é¡¯ç¤ºä¸åŒçš„æ¨¡å‹é¸é …
    if selected_platform == "huggingface":
        render_huggingface_models(current_config, api_url)
    else:
        render_ollama_models(current_config)
    
    # RAG æ¨¡å¼é¸æ“‡ - éš±è—ï¼Œå› ç‚ºå·²ç¶“åœ¨ä¸»ç•Œé¢é¡¯ç¤º
    # st.sidebar.markdown("### ğŸ”§ RAG æ¨¡å¼")
    # rag_options = {
    #     "traditional": "ğŸ“š å‚³çµ± RAG (å¿«é€Ÿ)",
    #     "dynamic": "âš¡ å‹•æ…‹ RAG (éˆæ´»)"
    # }
    # 
    # selected_rag_mode = st.sidebar.selectbox(
    #     "é¸æ“‡ RAG æ¨¡å¼:",
    #     options=list(rag_options.keys()),
    #     format_func=lambda x: rag_options[x],
    #     index=0 if current_config.get("rag_mode") == "traditional" else 1,
    #     key="model_selector_rag_mode"
    # )
    
    # ä½¿ç”¨é»˜èªçš„ RAG æ¨¡å¼ï¼ˆç”±æ–¼ä¸»ç•Œé¢å·²ç¶“è™•ç†ï¼‰
    selected_rag_mode = "traditional"
    
    # èªè¨€é¸æ“‡ - éš±è—ï¼Œå› ç‚ºåœ¨ä¸»ç•Œé¢å·²ç¶“è™•ç†
    # st.sidebar.markdown("### ğŸŒ èªè¨€è¨­ç½®")
    # language_options = {
    #     "traditional_chinese": "ğŸ‡¹ğŸ‡¼ ç¹é«”ä¸­æ–‡",
    #     "simplified_chinese": "ğŸ‡¨ğŸ‡³ ç®€ä½“ä¸­æ–‡",
    #     "english": "ğŸ‡ºğŸ‡¸ English",
    #     "thai": "ğŸ‡¹ğŸ‡­ à¹„à¸—à¸¢",
    #     "dynamic": "ğŸŒ è‡ªå‹•æª¢æ¸¬"
    # }
    # 
    # selected_language = st.sidebar.selectbox(
    #     "é¸æ“‡èªè¨€:",
    #     options=list(language_options.keys()),
    #     format_func=lambda x: language_options[x],
    #     index=4,  # é»˜èªè‡ªå‹•æª¢æ¸¬
    #     key="model_selector_language_selector"
    # )
    
    # ä½¿ç”¨é»˜èªèªè¨€ï¼ˆç”±æ–¼ä¸»ç•Œé¢å·²ç¶“è™•ç†ï¼‰
    selected_language = "dynamic"
    
    # ä¿å­˜é…ç½®æŒ‰éˆ•
    if st.sidebar.button("ğŸ’¾ ä¿å­˜é…ç½®", key="save_config"):
        new_config = {
            "platform": selected_platform,
            "language_model": st.session_state.get("selected_language_model"),
            "embedding_model": st.session_state.get("selected_embedding_model"),
            "inference_engine": st.session_state.get("selected_inference_engine", "transformers"),
            "rag_mode": selected_rag_mode,
            "language": selected_language,
            "setup_completed": True
        }
        
        if save_user_config(new_config):
            st.sidebar.success("âœ… é…ç½®å·²ä¿å­˜")
            # é€šçŸ¥ API æ›´æ–°é…ç½®
            try:
                response = requests.post(f"{api_url}/api/config/update", json=new_config)
                if response.status_code == 200:
                    st.sidebar.success("âœ… ç³»çµ±é…ç½®å·²æ›´æ–°")
                else:
                    st.sidebar.warning("âš ï¸ ç³»çµ±é…ç½®æ›´æ–°å¤±æ•—")
            except:
                st.sidebar.warning("âš ï¸ ç„¡æ³•é€£æ¥åˆ° API")
        else:
            st.sidebar.error("âŒ é…ç½®ä¿å­˜å¤±æ•—")

def render_huggingface_models(current_config: Dict[str, Any], api_url: str = "http://localhost:8000"):
    """æ¸²æŸ“ Hugging Face æ¨¡å‹é¸é …"""
    
    try:
        # å¾ API ç²å–å¯ç”¨æ¨¡å‹
        response = requests.get(f"{api_url}/api/setup/models", timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", {})
            
            # èªè¨€æ¨¡å‹
            st.sidebar.markdown("#### ğŸ¤– èªè¨€æ¨¡å‹")
            language_models = models.get("language_models", [])
            
            if language_models:
                # å‰µå»ºé¸é …å­—å…¸
                hf_language_models = {}
                for model in language_models:
                    display_name = f"{model['name']} ({model['size']})"
                    hf_language_models[model["id"]] = display_name
                
                selected_language_model = st.sidebar.selectbox(
                    "é¸æ“‡èªè¨€æ¨¡å‹:",
                    options=list(hf_language_models.keys()),
                    format_func=lambda x: hf_language_models[x],
                    index=0,
                    key="hf_language_model"
                )
                
                st.session_state.selected_language_model = selected_language_model
            else:
                st.sidebar.error("æ²’æœ‰æ‰¾åˆ°æœ¬åœ°èªè¨€æ¨¡å‹")
                st.sidebar.info("è«‹å…ˆä¸‹è¼‰æ¨¡å‹")
                st.sidebar.code("hf download Qwen/Qwen2-0.5B-Instruct --cache-dir ./models/cache")
            
            # åµŒå…¥æ¨¡å‹
            st.sidebar.markdown("#### ğŸ”¤ åµŒå…¥æ¨¡å‹")
            embedding_models = models.get("embedding_models", [])
            
            if embedding_models:
                # å‰µå»ºé¸é …å­—å…¸
                hf_embedding_models = {}
                for model in embedding_models:
                    display_name = f"{model['name']} ({model['size']})"
                    hf_embedding_models[model["id"]] = display_name
                
                selected_embedding_model = st.sidebar.selectbox(
                    "é¸æ“‡åµŒå…¥æ¨¡å‹:",
                    options=list(hf_embedding_models.keys()),
                    format_func=lambda x: hf_embedding_models[x],
                    index=0,
                    key="hf_embedding_model"
                )
                
                st.session_state.selected_embedding_model = selected_embedding_model
            else:
                st.sidebar.error("æ²’æœ‰æ‰¾åˆ°æœ¬åœ°åµŒå…¥æ¨¡å‹")
                st.sidebar.info("è«‹å…ˆä¸‹è¼‰æ¨¡å‹")
                st.sidebar.code("hf download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --cache-dir ./models/cache")
        
        else:
            st.sidebar.error("ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨")
            # ä½¿ç”¨å‚™ç”¨çš„ç¡¬ç·¨ç¢¼åˆ—è¡¨
            _render_fallback_huggingface_models()
    
    except Exception as e:
        st.sidebar.error(f"ç²å–æ¨¡å‹åˆ—è¡¨æ™‚å‡ºéŒ¯: {str(e)}")
        # ä½¿ç”¨å‚™ç”¨çš„ç¡¬ç·¨ç¢¼åˆ—è¡¨
        _render_fallback_huggingface_models()

def _render_fallback_huggingface_models():
    """æ¸²æŸ“å‚™ç”¨çš„ Hugging Face æ¨¡å‹é¸é …ï¼ˆAPI å¤±æ•—æ™‚ï¼‰"""
    st.sidebar.markdown("#### ğŸ¤– èªè¨€æ¨¡å‹")
    st.sidebar.error("ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨")
    st.sidebar.info("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼š")
    st.sidebar.code("hf download Qwen/Qwen2-0.5B-Instruct --cache-dir ./models/cache")
    
    # åµŒå…¥æ¨¡å‹
    st.sidebar.markdown("#### ğŸ”¤ åµŒå…¥æ¨¡å‹")
    st.sidebar.error("ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨")
    st.sidebar.info("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼š")
    st.sidebar.code("hf download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --cache-dir ./models/cache")
    
    # è¨­ç½®ç©ºçš„é¸æ“‡ç‹€æ…‹
    st.session_state.selected_language_model = None
    st.session_state.selected_embedding_model = None
    
    # æ¨ç†å¼•æ“
    st.sidebar.markdown("#### âš™ï¸ æ¨ç†å¼•æ“")
    inference_engines = {
        "transformers": "ğŸ”§ Transformers (ç©©å®š)",
        "vllm": "âš¡ vLLM (é«˜æ€§èƒ½)"
    }
    
    selected_inference_engine = st.sidebar.selectbox(
        "é¸æ“‡æ¨ç†å¼•æ“:",
        options=list(inference_engines.keys()),
        format_func=lambda x: inference_engines[x],
        index=0,
        key="hf_inference_engine"
    )
    
    st.session_state.selected_inference_engine = selected_inference_engine

def render_ollama_models(current_config: Dict[str, Any]):
    """æ¸²æŸ“ Ollama æ¨¡å‹é¸é …"""
    
    # èªè¨€æ¨¡å‹
    st.sidebar.markdown("#### ğŸ¤– èªè¨€æ¨¡å‹")
    ollama_language_models = {
        "llama3.2:3b": "Llama 3.2 3B (2GB)",
        "llama3.1:8b": "Llama 3.1 8B (4.7GB)",
        "llama3.1:70b": "Llama 3.1 70B (40GB)",
        "qwen2.5:7b": "Qwen 2.5 7B (4.4GB)",
        "gemma2:9b": "Gemma 2 9B (5.4GB)"
    }
    
    selected_language_model = st.sidebar.selectbox(
        "é¸æ“‡èªè¨€æ¨¡å‹:",
        options=list(ollama_language_models.keys()),
        format_func=lambda x: ollama_language_models[x],
        index=0,
        key="ollama_language_model"
    )
    
    st.session_state.selected_language_model = selected_language_model
    
    # åµŒå…¥æ¨¡å‹
    st.sidebar.markdown("#### ğŸ”¤ åµŒå…¥æ¨¡å‹")
    ollama_embedding_models = {
        "nomic-embed-text": "ğŸŒ Nomic Embed Text (274MB) - å¤šèªè¨€æ¨è–¦",
        "mxbai-embed-large": "ğŸš€ MxBai Embed Large (669MB) - é«˜ç²¾åº¦"
    }
    
    selected_embedding_model = st.sidebar.selectbox(
        "é¸æ“‡åµŒå…¥æ¨¡å‹:",
        options=list(ollama_embedding_models.keys()),
        format_func=lambda x: ollama_embedding_models[x],
        index=0,
        key="ollama_embedding_model"
    )
    
    st.session_state.selected_embedding_model = selected_embedding_model
    
    # Ollama åªä½¿ç”¨è‡ªå·±çš„æ¨ç†å¼•æ“
    st.session_state.selected_inference_engine = "ollama"

def get_current_config():
    """ç²å–ç•¶å‰é…ç½®"""
    return load_user_config()

def is_setup_completed():
    """æª¢æŸ¥æ˜¯å¦å®Œæˆè¨­ç½®"""
    config = load_user_config()
    return config.get("setup_completed", False)
